{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "789a088e11018ecf46b911f42eb7fb535bcdef5d93733b18d9688d03"
   },
   "source": [
    "# COMP90042 Assignment #1: Sentiment analysis for tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "signature": "807b47a7e2ba3738211fed67899b0887689deedc7a0fa66c0eae3617"
   },
   "source": [
    "Student Name: Cong Yue\n",
    "\n",
    "Student ID: 682020\n",
    "\n",
    "***Notice***: \n",
    "\n",
    "****I zipped train.json, dev.json and test.json with this script. If you demonstrate it, please run in the extracted directory or place all json file and this script in the same directory and then run****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "58dcfed543d6944e83853f201894729b6cfa2b6d21bd652954f7f44f"
   },
   "source": [
    "## General info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "875d1c558b2c216775270b7335d5d8c60977d6a8ed66936b11a68d57"
   },
   "source": [
    "<b>Due date</b>: 5pm, Mon April 12\n",
    "\n",
    "<b>Submission method</b>: see LMS\n",
    "\n",
    "<b>Submission materials</b>: completed copy of this ipython notebook\n",
    "\n",
    "<b>Late submissions</b>: -10% per day, no late submissions after the first week\n",
    "\n",
    "<b>Marks</b>: 25% of mark for class\n",
    "\n",
    "<b>Overview</b>: For this project, you'll be building a 3-way polarity classification system for tweets, using a logistic regression classifier, BOW features, as well as polarity lexicons built from external sources. A key focus of this project is critical analysis and experimental evaluation, for which you will need to report on the relative merits of various options. \n",
    "\n",
    "<b>Materials</b>: See the main class LMS page for information on the basic setup required for this class, including an iPython notebook viewer and the python packages NLTK, Numpy, Scipy, Matplotlib, Sci-kit Learn, and Gemsim. In particular, if you are not using a lab computer which already has it installed, we recommend installing all the data for NLTK, since you will need various parts of it to complete this assignment. You can also use any Python build-in packages, but do not use any other 3rd party packages; if your iPython notebook doesn't run on the marker's machine, you will lose marks. You are encouraged to use the iPython notebooks released for this class as well as other online documentation to guide your responses, but you should not copy directly from any source. The only other data you will need is three sets of tagged tweets, the first two of which (training and dev) were released at the same time as this notebook, and a third set (test) which will be made available about a week before the assignment is due, see Final Testing below. This data comes from the recent SemEval 2016 shared task. Do not distribute this data indiscriminately (i.e. put it on a public website), you should use it only for this assignment, and delete it afterwards. The corpus is comprised of unfiltered text from the web, and may include offensive or objectionable material. This reflects the general composition of the web and the general challenges present in web based text analysis. The University of Melbourne takes no responsibility for opinions expressed in the corpus, nor takes any responsibility for offence caused by these documents.\n",
    "\n",
    "<b>Evaluation</b>: Your iPython notebook should run end-to-end without any errors in a reasonable amount of time (less than 10 minutes on a lab desktop), and you must follow all instructions provided below, including specific implementation requirements. You will be marked not only on the correctness of your methods, but also on your explanation and analysis. Please do not change any of instruction text in the notebook. Where applicable, leave the output cells in the code, particularly when you are commenting on that output. You should add your answers and code by inserting a markdown cell between every major function or other block of code explaining its purpose or anywhere a result needs to be discussed (see the class notebooks for examples). Note that even if you do something wrong, you might get partial credit if you explain it enough that we can follow your reasoning, whereas a fully correct assignment with no text commentary will not receive a passing score. You will not be marked directly on the performance of your final classifier, but each of the steps you take to build it should be reasonable and well-defended.\n",
    "\n",
    "<b>Updates</b>: Any major changes to the assignment will be announced via LMS. Minor changes and clarifications will be announced in the forum on LMS, we recommend you check the forum regularly.\n",
    "\n",
    "<b>Academic Misconduct</b>: For most people, collaboration will form a natural part of the undertaking of this project, and we encourage you to discuss it in general terms with other students. However, it is still an individual task, and so reuse of code or other instances of clear influence will be considered cheating. We will be checking submissions for originality and will invoke the Universityâ€™s <a href=\"http://academichonesty.unimelb.edu.au/policy.html\">Academic Misconduct policy</a> where inappropriate levels of collusion or plagiarism are deemed to have taken place.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "1e3d7d2aeb24e2a301b7f36ea396f4bc0575704bcbc264bc1633150e"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "signature": "ad1cb0c46e858dbadc85f39823285e22c986dea9d331a2a8dbc501fa"
   },
   "source": [
    "<b>Instructions</b>: Your first task is to carry out preprocessing on the tweets. Use the code below as a starter. Each line of the input files is a json including the tweet and the label (and the tweet id), this code just loads them into a list without any preprocessing. Note that for the labels, 1 = positive, 0 = neutral, -1 = negative. Here is a list of things your preprocessing code must do:\n",
    "\n",
    "<ul>\n",
    "<li>Segment into sentences: Use NLTK punkt sentence segmenter</li>\n",
    "<li>Tokenize sentences: Use the NLTK regex WordPunct tokenizer</li>\n",
    "<li>Lowercase all words</li>\n",
    "<li>Remove Twitter usernames: Usernames on twitter begin with @</li>\n",
    "<li>Remove URLs: URLs start with http</li> \n",
    "<li>Remove any hashtags from their original location in the tweet, tokenize them, and add them as a separate sentences with the hash tag removed: for tokenization, use capitalized letters when they occur (e.g. #RefugeesWelcome -> Refugees Welcome), or when there is no capitalization (#refugeeswelcome -> refugees welcome) use the MaxMatch algorithm and the list of English words included in NLTK (nltk.corpus.words.words()). Two notes about the English word list: 1. you should convert it to a python set before you use it (sets are hashed, so you get much quicker lookup) 2. It contains only base forms, so you will need to lemmatize words before you look them up.</li>\n",
    "</ul>\n",
    "\n",
    "You can do these in almost any order you like, but it may be useful to do the main segmentation/tokenization last (or almost last), since for the other tasks it is easier to deal with the raw string rather than a list of tokens. The use of regular expressions is recommended, but not required.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "signature": "7187a5341cee7ea361eadead27ee01cc187f7df8062e822c661a3933"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "words = nltk.corpus.words.words()\n",
    "words = frozenset(words)\n",
    "\n",
    "username_url_pattern = re.compile('@\\S+|http://\\S+|https://\\S+')\n",
    "hashtag_pattern = re.compile('#\\w+')\n",
    "cap_token_pattern = re.compile('[A-Z][a-z0-9_]*')\n",
    "\n",
    "def preprocess(tweet):\n",
    "    # remove usernames, URLs\n",
    "    tweet = re.sub(username_url_pattern, '', tweet).strip()\n",
    "    \n",
    "    # remove hashtags\n",
    "    tweet = remove_hashtags(tweet, words)\n",
    "    \n",
    "    # lowercase all words\n",
    "    tweet = tweet.lower()\n",
    "\n",
    "    # segment into sentences\n",
    "    sgmter = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    tweet = sgmter.tokenize(tweet)\n",
    "    \n",
    "    # tokenize sentences\n",
    "    tokenizer = nltk.tokenize.regexp.WordPunctTokenizer()\n",
    "    tweet = [tokenizer.tokenize(sentence) for sentence in tweet]\n",
    "    \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanations**: \n",
    "\n",
    "It is easier to do case transformation, username and URL removal than sentence segmentation and tokenization. But the case transformation has to be set after hashtag removal, because when removing hashtags, it needs the feature of capitalized letter to tokenize sentence. So I set username and URL removal firstly, then hashtag removal and case transformation, finally do the segmentation and tokenization by using nltk. The order of preprocessing tasks is as the following list starts with **bullet point**\n",
    "\n",
    "* Remove Twitter usernames\n",
    "* Remove URLs\n",
    "\n",
    "The instruction requires to remove URLs start with **http**, we thought the aim is to remove non-readable text. So we consider it to be reasonable to remove **https** as well.\n",
    "\n",
    "When it comes to hashtags removal, it is not only character removal (#), but also tokenization and lemmatization need to be taken into account. However, tokenization here is different from tokenizing sentences by using nltk, we need to apply max match algorithm, so we operate hashtags' removal ahead of general sentence tokenization. There are two advantages, one is that we do not need to iterates all tokens inside a sentence to find hashtags, which means we reduce the number of iteration; the second is that after nltk regex tokenization, hashtag will be seperated with tagged sentences, which makes the sentence hard to find and tokenize. \n",
    "\n",
    "* Remove and tokenize hashtags.\n",
    "* Lowercase all words\n",
    "* Segment into sentences\n",
    "* Tokenize sentences\n",
    "\n",
    "We pre-compile the regex pattern to speed up the process. This is because we avoid compiling pattern inside the iteration, which is time-consuming. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_hashtags(tweet, words):\n",
    "    tagged_sentences = re.findall(hashtag_pattern, tweet)\n",
    "    for sentence in tagged_sentences:\n",
    "        # start from pos 1 to ignore the first '#' character.\n",
    "        target_sentence = sentence[1:]\n",
    "        # Use capitalized letter when it occurs\n",
    "        cap_tokens = re.findall(cap_token_pattern, target_sentence)\n",
    "        if cap_tokens:\n",
    "            untagged_sentence = ' '.join(cap_tokens)\n",
    "        # use max match and word dictionary to segment hashtagged topics\n",
    "        else:\n",
    "            result = max_match(target_sentence, words)\n",
    "            # remove extra space that produced by max_match at the end.\n",
    "            untagged_sentence = result.strip()\n",
    "        # replace tagged sentence with tokenized sentence\n",
    "        tweet = re.sub(sentence, untagged_sentence, tweet)\n",
    "        \n",
    "    return tweet\n",
    "\n",
    "def max_match(sentence, dic):\n",
    "    if len(sentence) == 0:\n",
    "        return ''\n",
    "    \n",
    "    for idx in xrange(len(sentence),0,-1):\n",
    "        # back traverse the sentence.\n",
    "        token = sentence[:idx]\n",
    "        reminder = sentence[idx:]\n",
    "        # compare using lemmatized tokens\n",
    "        lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "        lemma = lemmatizer.lemmatize(token)\n",
    "        if lemma in dic:\n",
    "            return '%s %s' % (token, max_match(reminder, dic))\n",
    "        \n",
    "    # no one in dic, one character word\n",
    "    token = sentence[:1]\n",
    "    reminder = sentence[1:]\n",
    "    return '%s %s' % (token, max_match(reminder, dic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "\n",
    "We seperate hashtag removal into 3 sections:\n",
    "\n",
    "1. Find substring that starts with '#' followed by printable characters. For single '#' characters and '#' followed by non-alphabet characters, we do not regard them as hashtag for a topic.\n",
    "2. Check if there are capital letters that can seperate the target sentence\n",
    "3. If there is no capital letter, seperate target sentence by using max_match function and word dictionary.\n",
    "\n",
    "We implement recursive version of max match algorithm, and compare the lemma with word in dictionary as required in **instruction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_file(filename):\n",
    "    tweets = []\n",
    "    labels = []\n",
    "    f = open(filename)\n",
    "    for line in f:\n",
    "        tweet_dict = json.loads(line)\n",
    "        tweets.append(preprocess(tweet_dict[\"text\"]))\n",
    "        labels.append(int(tweet_dict[\"label\"]))\n",
    "    return tweets,labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "6a0a44bc91c85a72d4ba4571741fe4e0aac3088ae49a80efed6d1233"
   },
   "source": [
    "<b>Instructions</b>: Once your basic preprocessing module is working, run it on the training set and have it print out 10 examples where your system identified a hashtag with more than one word inside; print out both the original tweet string as well as result after preprocessing. It's okay if you have to duplicate some code from above to do this. Point out any errors you see in the preprocessing, and discuss possible solutions; these can be related to the hashtags, or any other errors you see. You do not have to fix the errors unless they actually indicate a actual bug in your code (at which point you should go back to the previous section, fix the code, and print out the samples again)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false,
    "signature": "5c169632516a1496704cb5ea5082ff3ca3e5c0c69940b9b0ecce4876"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. ORIGINAL TWEET [16]: #teens @BillGates 1st company failed miserably. When Gates & @PaulGAllen tried to sell the product it wouldn't work #nevergiveup @Microsoft\n",
      "0. PROCESSED TWEET [16]: [[u'teens', u'1st', u'company', u'failed', u'miserably', u'.'], [u'when', u'gates', u'&', u'tried', u'to', u'sell', u'the', u'product', u'it', u'wouldn', u\"'\", u't', u'work', u'never', u'give', u'up']]\n",
      "\n",
      "1. ORIGINAL TWEET [17]: #Vote for @AIESEC to become the 10th Global non profit partner of @Microsoft for us to #UpgradeYourWorld together. @AIESECGermany\n",
      "1. PROCESSED TWEET [17]: [[u'vote', u'for', u'to', u'become', u'the', u'10th', u'global', u'non', u'profit', u'partner', u'of', u'for', u'us', u'to', u'upgrade', u'your', u'world', u'together', u'.']]\n",
      "\n",
      "2. ORIGINAL TWEET [70]: #llvm 3.7 is the 2nd release to incorporate contributions from my team at @microsoft. Congrats to all involved! :) https://t.co/IUtbNxonfw\n",
      "2. PROCESSED TWEET [70]: [[u'l', u'l', u'v', u'm', u'3', u'.', u'7', u'is', u'the', u'2nd', u'release', u'to', u'incorporate', u'contributions', u'from', u'my', u'team', u'at', u'congrats', u'to', u'all', u'involved', u'!'], [u':)']]\n",
      "\n",
      "3. ORIGINAL TWEET [88]: #Producers Catch me in Dallas Sept 12th at the @Microsoft event.  Also at @A3C in #ATL for the On The Spot Beat Shopping!\n",
      "3. PROCESSED TWEET [88]: [[u'producers', u'catch', u'me', u'in', u'dallas', u'sept', u'12th', u'at', u'the', u'event', u'.'], [u'also', u'at', u'in', u'a', u't', u'l', u'for', u'the', u'on', u'the', u'spot', u'beat', u'shopping', u'!']]\n",
      "\n",
      "4. ORIGINAL TWEET [222]: #RT The 3rd book in The Viking's Apprentice series has been released. Amazon users get it here http://t.co/0pJjo6xXN7 #amreading\n",
      "4. PROCESSED TWEET [222]: [[u'r', u't', u'the', u'3rd', u'book', u'in', u'the', u'viking', u\"'\", u's', u'apprentice', u'series', u'has', u'been', u'released', u'.'], [u'amazon', u'users', u'get', u'it', u'here', u'am', u'reading']]\n",
      "\n",
      "5. ORIGINAL TWEET [401]: #TheVerge Amazon Prime Day is taking over Black Friday: Amazon has created its own holiday. The company just a... http://t.co/fonw146CDG\n",
      "5. PROCESSED TWEET [401]: [[u'the', u'verge', u'amazon', u'prime', u'day', u'is', u'taking', u'over', u'black', u'friday', u':', u'amazon', u'has', u'created', u'its', u'own', u'holiday', u'.'], [u'the', u'company', u'just', u'a', u'...']]\n",
      "\n",
      "6. ORIGINAL TWEET [503]: #Germany's leader Angela Merkel @Angie_Merkel  to run for 4th term, she is in talks about her election campaign according to #DerSpiegel\n",
      "6. PROCESSED TWEET [503]: [[u'germany', u\"'\", u's', u'leader', u'angela', u'merkel', u'to', u'run', u'for', u'4th', u'term', u',', u'she', u'is', u'in', u'talks', u'about', u'her', u'election', u'campaign', u'according', u'to', u'der', u'spiegel']]\n",
      "\n",
      "7. ORIGINAL TWEET [606]: #apple A look at the evolution of Apple's iPhone: Apple will unveil the new iPhone on Wednesday. The next iPho... http://t.co/TEDe4E4fuA\n",
      "7. PROCESSED TWEET [606]: [[u'apple', u'a', u'look', u'at', u'the', u'evolution', u'of', u'apple', u\"'\", u's', u'iphone', u':', u'apple', u'will', u'unveil', u'the', u'new', u'iphone', u'on', u'wednesday', u'.'], [u'the', u'next', u'ipho', u'...']]\n",
      "\n",
      "8. ORIGINAL TWEET [624]: #AppleEvent is about to start! Watch Apple's live stream here: http://t.co/pItt3mUI4B\n",
      "8. PROCESSED TWEET [624]: [[u'apple', u'event', u'is', u'about', u'to', u'start', u'!'], [u'watch', u'apple', u\"'\", u's', u'live', u'stream', u'here', u':']]\n",
      "\n",
      "9. ORIGINAL TWEET [640]: #Tech Apple announces major update to the Apple Watch for September 16th http://t.co/HxjB93UKpg http://t.co/45hl0h59hZ\n",
      "9. PROCESSED TWEET [640]: [[u'tech', u'apple', u'announces', u'major', u'update', u'to', u'the', u'apple', u'watch', u'for', u'september', u'16th']]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idx = 0 # tweet index in tweets, map each processed tweet with original tweet\n",
    "ptr = 0 # pointer to tweet that has hashtags.\n",
    "tweets, labels = preprocess_file('train.json')\n",
    "f = open('./train.json')\n",
    "for line in f:\n",
    "    # print out 10 examples with hashtags\n",
    "    if ptr >= 10:\n",
    "         break\n",
    "    tweet_dict = json.loads(line)\n",
    "    if re.match(hashtag_pattern, tweet_dict['text']):\n",
    "        print '%s. ORIGINAL TWEET [%s]: %s' % (ptr, idx, tweet_dict['text'])\n",
    "        print '%s. PROCESSED TWEET [%s]: %s' % (ptr, idx, tweets[idx])\n",
    "        print \n",
    "        ptr = ptr + 1\n",
    "    idx = idx + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**: There is an error in the 503th (start from 0, also the 6th from example above) tweet. There is an error on regular expression pattern of username removal. The original tweet has '@Angie_Merkel', the result only remove 'Angie' and left '_Merkel'. The original regex is '@[a-zA-Z0-9]+', the solution is '@\\S+'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "993c11defaebeb96f1cc09cb29d53694610291ed712755d047edb825"
   },
   "source": [
    "<b>Instructions</b>: The next step will be to convert each of your preprocessed tweets into a feature dictionary, that is, a python dictionary where each entry corresponds to a feature and its value. At this stage, you should just build a bag-of-word feature dict, though you must allow for two possible options: one is to remove stopwords (using the NLTK stopword list), and the other is to remove words appearing <em>less</em> than n times across the entire training set (n<=0 should have no effect). The outer function (convert_to_feature dicts) should take the list of tweets resulting from the preprocess_file, and return a list of feature dictionaries in the same order (so they correspond to the label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "signature": "9735e8a4c6a48e7c95f7d367e4cc79ac4f5e66114d2f8ddfb1d73f4f"
   },
   "outputs": [],
   "source": [
    "def convert_to_feature_dicts(tweets,remove_stop_words=set(),n=0):\n",
    "    feature_dicts = []\n",
    "    # record the occurance of tokens based on entire training set\n",
    "    global_feature_dict = {}\n",
    "    for tweet in tweets:\n",
    "        feature_dict = {}\n",
    "        for sentence in tweet:\n",
    "            for token in sentence:\n",
    "                # skip stop words\n",
    "                if token in remove_stop_words:\n",
    "                    continue \n",
    "                # add token into dict\n",
    "                if token not in feature_dict:\n",
    "                    feature_dict.setdefault(token, 1)\n",
    "                else:\n",
    "                    feature_dict[token] = feature_dict[token] + 1\n",
    "                # add token to global dictionary preparing for removing less frequency tokens\n",
    "                if token not in global_feature_dict:\n",
    "                    global_feature_dict.setdefault(token, 1)\n",
    "                else:\n",
    "                    global_feature_dict[token] = global_feature_dict[token] + 1\n",
    "        feature_dicts.append(feature_dict)\n",
    "    \n",
    "    if n <= 0:\n",
    "        return feature_dicts\n",
    "    \n",
    "    # remove feature that occur less than n times\n",
    "    for idx, dic in enumerate(feature_dicts):\n",
    "        for feature in dic.keys():\n",
    "            if global_feature_dict[feature] <= n:\n",
    "                feature_dicts[idx].pop(feature)\n",
    "             \n",
    "    return feature_dicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**: \n",
    "\n",
    "According to my implementation, a tweet in tweets is a list and each sentence in a tweet is a list, so I need 3 nested for loop to traverse every tokens in a sentence.\n",
    "\n",
    "`Remove stop words`\n",
    "\n",
    "Build a set (stopwords) of stop words, skip next actions if tokens are in stopwords.\n",
    "\n",
    "`Remove less frequent words`\n",
    "\n",
    "Build a global dictionary that records all words' fequency based on entire training set. Use a for loop to remove less frequent feature from feature_dicts one by one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "bf26058cdb7c9612e552d173bba26af7d6876a04e7efce2cc5af9843"
   },
   "source": [
    "## Tuning and classifying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "8b6efe6a6c073b645cc52f0811fe7d94fe0d656af64816e48748d7a1"
   },
   "source": [
    "<b>Instructions</b>: Using the functions you've written, you should produce lists of feature dictionaries for both training and development sets; for the training set, remove stopwords and all words that appear only once (do <em>not</em> this for the dev set). Using scikit learn, convert the data to the sparse representation used for training classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "signature": "09edf385c287991c866cd5df30ff4b995dd6b4b917e15026dfc31e29"
   },
   "outputs": [],
   "source": [
    "stopwords = frozenset(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "train_set = 'train.json'\n",
    "train_tweets, train_labels = preprocess_file(train_set)\n",
    "train_feature_dicts = convert_to_feature_dicts(train_tweets, stopwords, 1)\n",
    "\n",
    "dev_set = 'dev.json'\n",
    "dev_tweets, dev_labels = preprocess_file(dev_set)\n",
    "dev_feature_dicts = convert_to_feature_dicts(dev_tweets)\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "vectorizer = DictVectorizer()\n",
    "train_data = vectorizer.fit_transform(train_feature_dicts)\n",
    "dev_data = vectorizer.transform(dev_feature_dicts)\n",
    "\n",
    "import numpy as np\n",
    "train_labels = np.asarray(train_labels)\n",
    "dev_labels = np.asarray(dev_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "1de01fffe89d7da22c3a77add4d511f05236af609acc274af1a73ef6"
   },
   "source": [
    "<b>Instructions</b>: Now, tune a decision tree classifier using accuracy in the development set as the evaluation metric. For this, you need to consider at least 2 parameters of the model likely to influence performance and which make sense in this context; you should read the documentation for the classifier on sci-kit learn website to learn what these parameters are. For any binary or categorical parameters, you should just consider all options. For numerical values, you should start by keep other settings on default and just randomly try a wide range, looking for values above which there is a steep drop-off in performance, or, alternatively, no effect on performance at all (you don't need to show this process in the notebook).  Remember that some parameters should be tested on a logarithmic scale. Once you're fairly confident of a good range for the parameter, divide it up into at least 5 steps (but no more than 10), and carry out a grid search, which is to say an exhaustive exploration of all parameter options within the limits you've set (this should be included in the notebook). Identify the best parameter values, and discuss the influence of the parameters on performance in the development set. Do you think some values of the parameters are resulting in overfitting?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "signature": "73166f5c30a1faac2ba23c3af899155e5e59e0e00516b06049aeb7f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[max_features: 0.9,    max_leaf_nodes: 2]\n",
      "Model: DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=0.9, max_leaf_nodes=2, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "Accuracy: 0.44669218152\n",
      "\n",
      "[max_features: 0.9,    max_leaf_nodes: 10]\n",
      "Model: DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=0.9, max_leaf_nodes=10, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "Accuracy: 0.48277747403\n",
      "\n",
      "[max_features: 0.9,    max_leaf_nodes: 50]\n",
      "Model: DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=0.9, max_leaf_nodes=50, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "Accuracy: 0.485511208311\n",
      "\n",
      "[max_features: 0.99,    max_leaf_nodes: 2]\n",
      "Model: DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=0.99, max_leaf_nodes=2, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "Accuracy: 0.44669218152\n",
      "\n",
      "[max_features: 0.99,    max_leaf_nodes: 10]\n",
      "Model: DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=0.99, max_leaf_nodes=10, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "Accuracy: 0.48277747403\n",
      "\n",
      "[max_features: 0.99,    max_leaf_nodes: 50]\n",
      "Model: DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=0.99, max_leaf_nodes=50, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "Accuracy: 0.485511208311\n",
      "\n",
      "The best parameters are: max_features - 0.9, \tmax_leaf_nodes - 50\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = DecisionTreeClassifier(criterion='gini',)\n",
    "\n",
    "# grid search params\n",
    "param_grid = {'max_features': [.9, .99],\n",
    "             'max_leaf_nodes': [2,10,50]}\n",
    "\n",
    "# accuracy to params dictionary\n",
    "acc_score_dict = {}\n",
    "for param_out in param_grid['max_features']:\n",
    "    for param_in in param_grid['max_leaf_nodes']:\n",
    "        clf.set_params(max_features=param_out,\n",
    "                       max_leaf_nodes=param_in)\n",
    "        clf.fit(train_data, train_labels)\n",
    "        preds = clf.predict(dev_data)\n",
    "        print '[max_features: %s,    max_leaf_nodes: %s]' % (param_out, param_in)\n",
    "        print 'Model: %s' % clf\n",
    "        acc = accuracy_score(dev_labels, preds)\n",
    "        print 'Accuracy: %s\\n' % acc\n",
    "        acc_score_dict.setdefault(acc, (param_out, param_in))\n",
    "print 'The best parameters are: max_features - %s, \\tmax_leaf_nodes - %s' % \\\n",
    "        (acc_score_dict[max(acc_score_dict.keys())][0], \n",
    "        acc_score_dict[max(acc_score_dict.keys())][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "\n",
    "The best parameter values of the pair params we choose is: `max_features`: 0.9, `max_leaf_nodes`: 50 \n",
    "\n",
    "The value of ***max_features*** decides what percentage of total features (in our case it is 10890) are used to split nodes. The more features are used for spliting a node, the more the tree tend to overfit, but too small of that value  underfit the groud truth on the contrary. So I choose '0.9', most of features for each node spliting, and '0.99', almost all of features to check whether it overfits. The result report above gives the positive answer, every example that max_features=0.99 has lower accuracy than the corresponding example ( example holding max_leaf_nodes static).\n",
    "\n",
    "The value of ***max_leaf_nodes*** defines the stop criteria of the growth of tree. The deeper the tree grows, the smaller the node that leaf contain, which means the more granular the model fit the observations. This is bad, the structure risk rises. As we can see from the result on development set, the smaller the value, the lower of the accuracy. I still do not have a clue about which is better, 10 or 50, and the grid search result gives me the answer: 50. However, this is not the larger the better. It underfits if the value is too small, because the tree depth will be very short."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "3d71c50b900df2f3ee10d073c3e1a069a3df91b89d7cbece1d299c25"
   },
   "source": [
    "<b>Instructions</b>: Carry out the same tuning process with the logistic regression classifier. Compare the performance of the two classifiers to each other, and to the most common class baseline. How are the classifiers doing? Is this a challenging task?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "signature": "ef2d9146a359cecd9de02b685c1cddb938c6bf4b8bc684b5c207a5fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C: 0.01,    max_iter: 10]\n",
      "Model: LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=10, multi_class='multinomial',\n",
      "          n_jobs=1, penalty='l2', random_state=None, solver='lbfgs',\n",
      "          tol=0.0001, verbose=0, warm_start=False)\n",
      "Accuracy: 0.498086386003\n",
      "\n",
      "[C: 0.01,    max_iter: 100]\n",
      "Model: LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='multinomial',\n",
      "          n_jobs=1, penalty='l2', random_state=None, solver='lbfgs',\n",
      "          tol=0.0001, verbose=0, warm_start=False)\n",
      "Accuracy: 0.492072170585\n",
      "\n",
      "[C: 0.1,    max_iter: 10]\n",
      "Model: LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=10, multi_class='multinomial',\n",
      "          n_jobs=1, penalty='l2', random_state=None, solver='lbfgs',\n",
      "          tol=0.0001, verbose=0, warm_start=False)\n",
      "Accuracy: 0.488791689448\n",
      "\n",
      "[C: 0.1,    max_iter: 100]\n",
      "Model: LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='multinomial',\n",
      "          n_jobs=1, penalty='l2', random_state=None, solver='lbfgs',\n",
      "          tol=0.0001, verbose=0, warm_start=False)\n",
      "Accuracy: 0.500273373428\n",
      "\n",
      "[C: 1,    max_iter: 10]\n",
      "Model: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=10, multi_class='multinomial',\n",
      "          n_jobs=1, penalty='l2', random_state=None, solver='lbfgs',\n",
      "          tol=0.0001, verbose=0, warm_start=False)\n",
      "Accuracy: 0.492072170585\n",
      "\n",
      "[C: 1,    max_iter: 100]\n",
      "Model: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='multinomial',\n",
      "          n_jobs=1, penalty='l2', random_state=None, solver='lbfgs',\n",
      "          tol=0.0001, verbose=0, warm_start=False)\n",
      "Accuracy: 0.492072170585\n",
      "\n",
      "The best parameters are: C - 0.1, \tmax_iter - 100\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(solver='lbfgs',\n",
    "                         multi_class='multinomial')\n",
    "\n",
    "# grid search params\n",
    "param_grid = {'C': [0.01, 0.1, 1],\n",
    "             'max_iter': [10, 100]}\n",
    "\n",
    "# accuracy to params dictionary\n",
    "acc_score_dict = {}\n",
    "for param_out in param_grid['C']:\n",
    "    for param_in in param_grid['max_iter']:\n",
    "        clf.set_params(C=param_out,\n",
    "                       max_iter=param_in)\n",
    "        clf.fit(train_data, train_labels)\n",
    "        preds = clf.predict(dev_data)\n",
    "        print '[C: %s,    max_iter: %s]' % (param_out, param_in)\n",
    "        print 'Model: %s' % clf\n",
    "        acc = accuracy_score(dev_labels, preds)\n",
    "        print 'Accuracy: %s\\n' % acc\n",
    "        acc_score_dict.setdefault(acc, (param_out, param_in))\n",
    "print 'The best parameters are: C - %s, \\tmax_iter - %s' % \\\n",
    "        (acc_score_dict[max(acc_score_dict.keys())][0], \n",
    "        acc_score_dict[max(acc_score_dict.keys())][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "\n",
    "Logistic regression is a bit better than decision tree classifier based on accuracy. We believe this is because decision tree is more tend to overfit given too many features (10890 in our case). We drove this by 2 reasons:\n",
    "\n",
    "1. Accuracy of cross validation on training set is better than accuracy on development set\n",
    "2. Accuracy increases a little when apply PCA to 1000 features before fit the training set.\n",
    "\n",
    "With chosen the best params, LogisticRegression classifier (acc: 49%) is slightly better than DecisionTreeClassifier (acc: 43%). Considering the baseline of 3 category classification problem baseline (acc: 33.3%), they are not bad but not impressive. Document classification is indeed a challenging task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "807e02e8c00d7896d78a72b6b18396a073c05a7927937c667dc931b2"
   },
   "source": [
    "<b>Instructions</b>: The next task is a slight detour to test your understanding of the logistic regression classifier: you are going to build your own classifier based on the trained model from sci-kit learn. In particular, you should fill in the MyLogisticRegression class started below which is initialized using the feature weights (coefficients) and constants (intercepts) and list of labels (classes) from the sci-kit learn classifier (see the \"Attributes\" in the documentation for the Logistic Regression classifier), and which mimics the predict and predict_proba methods from the sci-kit learn classifier object. You should confirm that your solution works by using it in the task at hand: take the classifier defined below, train it on the training data, then create an instance of MyLogisticRegression, and show that your classifier has the same output as the scikit-learn classifier for both predict and predict_proba for 5 samples from the development set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "signature": "629a209c0482571ac9af40d7b69c6a28b0fd935ae83cad045209ac4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression[Predictions of Top 5 samples]:\n",
      "[ 0  0  1 -1  1]\n",
      "MyLogisticRegression[Predictions of Top 5 samples]:\n",
      "[ 0  0  1 -1  1]\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "LogisticRegression[Probability of Top 5 samples]:\n",
      "[[ 0.05784951  0.92779904  0.01435145]\n",
      " [ 0.00201099  0.92458026  0.07340875]\n",
      " [ 0.3887802   0.17578476  0.43543504]\n",
      " [ 0.63965137  0.290759    0.06958963]\n",
      " [ 0.04370327  0.35161859  0.60467815]]\n",
      "MyLogisticRegression[Probability of Top 5 samples]:\n",
      "[[ 0.05784951  0.92779904  0.01435145]\n",
      " [ 0.00201099  0.92458026  0.07340875]\n",
      " [ 0.3887802   0.17578476  0.43543504]\n",
      " [ 0.63965137  0.290759    0.06958963]\n",
      " [ 0.04370327  0.35161859  0.60467815]]\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse.csr import csr_matrix\n",
    "\n",
    "class MyLogisticRegression:\n",
    "    \n",
    "    def __init__(self, weights, constants, labels):\n",
    "        # weights: n_class by n_feature matrix\n",
    "        # self.w_: n_class by n_bias+n_feature matrix\n",
    "        self.w_ = np.insert(weights, 0, values=constants, axis=1)\n",
    "        # n_class row vector\n",
    "        self.c = labels\n",
    "        # the number of classes\n",
    "        self.n_class = labels.shape[0]\n",
    "        \n",
    "\n",
    "    def predict_proba(self,X):     \n",
    "        # transform csr matrix to ndarray\n",
    "        X_dense = X.toarray()\n",
    "        # add bias\n",
    "        X_dense = np.insert(X_dense, 0, values=np.ones(X_dense.shape[0]), axis=1)\n",
    "        # back to csr_matrix\n",
    "        X = csr_matrix(X_dense)\n",
    "        \n",
    "        # exp(xw') / SUM(exp(xw'))\n",
    "        return np.exp(X * self.w_.T) / \\\n",
    "                np.repeat(np.sum(np.exp(X * self.w_.T), axis=1, keepdims=True), \n",
    "                          self.n_class, axis=1)\n",
    "        \n",
    "        \n",
    "    def predict(self,X):\n",
    "        # transform csr matrix to ndarray\n",
    "        X_dense = X.toarray()\n",
    "        # add bias\n",
    "        X_dense = np.insert(X_dense, 0, values=np.ones(X_dense.shape[0]), axis=1)\n",
    "        # back to csr_matrix\n",
    "        X = csr_matrix(X_dense)\n",
    "        \n",
    "        # exp(xw') comparison\n",
    "        pred_class_indices = np.argmax(np.exp(X * self.w_.T), axis=1)\n",
    "        return self.c[pred_class_indices]\n",
    "    \n",
    "    \n",
    "clf = LogisticRegression(solver='lbfgs', multi_class='multinomial')\n",
    "clf.fit(train_data, train_labels)\n",
    "\n",
    "my_clf = MyLogisticRegression(clf.coef_, clf.intercept_, clf.classes_)\n",
    "print \"LogisticRegression[Predictions of Top 5 samples]:\"\n",
    "print clf.predict(dev_data[:5,:])\n",
    "print \"MyLogisticRegression[Predictions of Top 5 samples]:\"\n",
    "print my_clf.predict(dev_data[:5,:])\n",
    "print '---------------------------------------------------------------------------------------------------------------'\n",
    "print \"LogisticRegression[Probability of Top 5 samples]:\"\n",
    "print clf.predict_proba(dev_data[:5,:])\n",
    "print \"MyLogisticRegression[Probability of Top 5 samples]:\"\n",
    "print my_clf.predict_proba(dev_data[:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "\n",
    "Considering the fact that all data are either ndarray type or matrix, I choose to directly use linear algebra operation to get the result. \n",
    "\n",
    "In constructor (`__init__`), we need to add intercept to weights.\n",
    "\n",
    "For function `predict_proba`, we need to firstly add intercept to `X`, then calculate the full softmax result, aka ***exp(xw') / SUM(exp(xw'))***.\n",
    "\n",
    "For function `predict`, we also firstly add intercept to `X`, then we only need to calculate ***exp(xw')***. Because for all three candidates, they share the same dinominator (`SUM(exp(xw'))`)\n",
    "\n",
    "A trick really consume me a lot of time is that training set and dev set are all ***csr matrix***, a compressed sparse matrix from `scipy`. If we want to add bias simply and logically, we need to transform it to ***dense matrix*** or ***ndarray***, add dummy, then transform it back to csr matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "9d9111395cbc372f1dd8f06e0d80e6ede9b3fcd44d8ade5c1722309d"
   },
   "source": [
    "## Polarity Lexicons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "89e30d51a6a2adecbed61927fff337db531dfb032a21657f5a642280"
   },
   "source": [
    "<b>Instructions</b>: Next we will try integrating information from sources beyond the training set, in the form of polarity lexicons. The main focus of this section is producing and evaluating 3 automatically-built polarity lexicons. The first of these lexicons is SentiWordNet, which is <a href=\"http://www.nltk.org/howto/sentiwordnet.html\"> accessible through NLTK</a>. SentiWordNet has precalculated scores for positive, negative, and neutral sentiment for some of the words in WordNet, but, like WordNet, it is arranged in synsets; building a WSD system to handle this is beyond the scope of this assignment, instead you should take the most common polarity across its senses (neutral if there is a tie). Do this by iterating through all the synsets in WordNet (which may take a little while, the code snippet below has a counter to show your progress), and then create two lists, one of positive words, one of negative words. Show 5 examples of each of the positive and negative words, and comment on their quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "signature": "e0b2441f54f52af9e1c4732ebf1d382d0106dec635577093bf47cf9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive: direct\t\t\tNegative: unable\n",
      "Positive: living\t\t\tNegative: relative\n",
      "Positive: ascetic\t\t\tNegative: comparative\n",
      "Positive: ascetical\t\t\tNegative: assimilating\n",
      "Positive: austere\t\t\tNegative: assimilative\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "\n",
    "count = 0\n",
    "# list that store positive sentiment synsets\n",
    "pos_words_swn = []\n",
    "# list that store negative sentiment synsets\n",
    "neg_words_swn = []\n",
    "\n",
    "def get_polarity_type(synset_name):\n",
    "    swn_synset =  swn.senti_synset(synset_name)\n",
    "    if not swn_synset:\n",
    "        return None\n",
    "    elif swn_synset.pos_score() > swn_synset.neg_score() and swn_synset.pos_score() > swn_synset.obj_score():\n",
    "        return 1\n",
    "    elif swn_synset.neg_score() > swn_synset.pos_score() and swn_synset.neg_score() > swn_synset.obj_score():\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "    \n",
    "for synset in wn.all_synsets():\n",
    "    count += 1\n",
    "        \n",
    "    if get_polarity_type(synset.name()) > 0:\n",
    "        for lemma in synset.lemma_names():\n",
    "            pos_words_swn.append(lemma)\n",
    "    elif get_polarity_type(synset.name()) < 0:\n",
    "        for lemma in synset.lemma_names():\n",
    "            neg_words_swn.append(lemma)\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "for i in xrange(5):\n",
    "    print 'Positive: %s\\t\\t\\tNegative: %s' % (pos_words_swn[i], neg_words_swn[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "\n",
    "From the definition of `WordNet` of a synset, all examples are classified reasonable. But some word, in a specific context, it is not always convincing. For example, the word ***austere*** is categorized as positive, while the definition in `WordNet` of `austere.s.01` is \"severely simple\", obviously negative. The counterpart example is ***assimilating*** that is neutral, not only according to human sense, but from the the defination in `WordNet` as illustrated in `assimilating.s.01`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "f85a3c2a409bc06239d47c31fbc20b7889824a9c37e41e550d621189"
   },
   "source": [
    "<b>Instructions</b>: The second lexicon will be built using the word2vec (CBOW) vectors included in NLTK. For this, you will need a small set of positive and negative seed terms, which are given to you below. Calculate cosine similarity between vectors of the seeds terms and each of the words for which you have vectors (if you use Gensim, you can iterate over model.vocab), flip the sign for the negative seeds, and then average to get a score. Use this score to produce a list of positive and negative words; you should include a threshold of Â±0.03 for words to be considered positive or negative. Again, show 5 examples of each of the positive and negative words, and comment on their quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "signature": "7305295a2e9e89c2e109557ce906214837fa83164b04d55055c7d606"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------Positive 5 examples--------------------------\n",
      "[u'loen', u'pergamon', u'feasibility', u'pampa', u'modest']\n",
      "-------------------------Negative 5 examples--------------------------\n",
      "[u'debts', u'clotted', u'hastily', u'comically', u'disobeying']\n"
     ]
    }
   ],
   "source": [
    "positive_seeds = [\"good\",\"nice\",\"excellent\",\"positive\",\"fortunate\",\"correct\",\"superior\",\"great\"]\n",
    "negative_seeds = [\"bad\",\"nasty\",\"poor\",\"negative\",\"unfortunate\",\"wrong\",\"inferior\",\"awful\"]\n",
    "\n",
    "from gensim import models\n",
    "sample = str(nltk.data.find('models/word2vec_sample/pruned.word2vec.txt'))\n",
    "model = models.Word2Vec.load_word2vec_format(sample, binary=False)\n",
    "    \n",
    "pos_words_cos = []\n",
    "neg_words_cos = []\n",
    "\n",
    "# build positive/negative word list\n",
    "for word in model.vocab:\n",
    "    pos_length, neg_length = len(positive_seeds), len(negative_seeds)\n",
    "    sim_array = np.zeros(pos_length + neg_length)\n",
    "    for idx, pos in enumerate(positive_seeds):\n",
    "        sim_array[idx] = model.similarity(pos, word)\n",
    "    for idx, neg in enumerate(negative_seeds):\n",
    "        # flip the sign\n",
    "        sim_array[pos_length+idx] = - model.similarity(neg, word)\n",
    "    # average the score\n",
    "    score = np.sum(sim_array) / len(sim_array)\n",
    "    if score >= 0.03:\n",
    "        pos_words_cos.append(word.lower())\n",
    "    elif score <= -0.03:\n",
    "        neg_words_cos.append(word.lower())\n",
    "        \n",
    "print '-------------------------Positive 5 examples--------------------------'\n",
    "print pos_words_cos[:5]\n",
    "print '-------------------------Negative 5 examples--------------------------'   \n",
    "print neg_words_cos[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "\n",
    "Considering the positive 5 example, only ***feasibility*** and ***modest*** are reasonably positive words. Other 3 words are either name of place or name of person, which is really hard to add polarity meaning to them. However, at least we can assume that this model tend to classify names to be positive based on examples, so we need to be cautious when using it.\n",
    "\n",
    "Examples from negative list are impressive, all 5 words have apparent nagative senses. \n",
    "\n",
    "If only refer the examples, it is recommended that we shall use negative word list from this model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "466d6a60e0221d5cbc396cdaed5ff809b08019c160f06cbd290695aa"
   },
   "source": [
    "<b>Instructions</b>: The third lexicon will be built by calculating PPMI with the seed terms. For this, use the Brown corpus included in NLTK, with co-occurrence defined as <em>binary</em> text co-occurrence (that is, multiple co-occurrences in the same text are not counted); importantly, your solution should <em>not</em> calculate the entire co-occurrence matrix, since you only care about relative co-occurrence with the seeds. As above, average the resulting similarity scores after switching the sign for the negative seeds and use them to produce a list of positive and negative words, and check 5 of each. For PPMI, use a threshold of  Â±0.3 for deciding if a word is neutral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "signature": "da511e2c332ffcf4b12e0910fd8bf20260decfd3d5901d4bb2971dee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------Positive 5 examples--------------------------\n",
      "[u'term-end', u'presentments', u'september-october', u'durwood', u'pye']\n",
      "-------------------------Negative 5 examples--------------------------\n",
      "[u'registration', u'pearl', u'1958', u'insure', u'smooth']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "from gensim import corpora\n",
    "from itertools import izip\n",
    "\n",
    "def get_pos_neg_words(positive_seeds, negative_seeds):\n",
    "    # build seed to id dictionary, id starts from 0\n",
    "    seed2id = dict(zip(positive_seeds+negative_seeds, \n",
    "                       range(len(positive_seeds) + len(negative_seeds))))\n",
    "\n",
    "    # build texts & word to id & id to word dictionary, all id starts from 0\n",
    "    word2id = {}\n",
    "    texts = []\n",
    "    for fileid in brown.fileids():\n",
    "        text = [word.lower() for word in brown.words(fileid)]\n",
    "        texts.append(text)\n",
    "        for word in text:\n",
    "            if word in seed2id:\n",
    "                # add only relavant word to dictionary\n",
    "                for token in text:\n",
    "                    if token not in word2id:\n",
    "                        word2id.setdefault(token, len(word2id))\n",
    "    id2word = dict(izip(word2id.itervalues(), word2id.iterkeys()))\n",
    "\n",
    "    # build a relavant word by seed matrix\n",
    "    word_seed_matrix = np.zeros((len(word2id), len(seed2id)))\n",
    "\n",
    "    # iterate all texts to udpate word-by-seed matrix\n",
    "    for text in texts:\n",
    "        distinct_seeds = set()\n",
    "        for word in text:\n",
    "            # ignore if seed never appears or appears more than once in the text\n",
    "            if word not in seed2id or word in distinct_seeds:\n",
    "                continue\n",
    "            for occur in text:\n",
    "                word_seed_matrix[word2id[occur], seed2id[word]] += 1.\n",
    "            # update dictinct_seeds \n",
    "            distinct_seeds.add(word)\n",
    "\n",
    "    # calc PPMI for word by seed matrix\n",
    "    count_all = np.sum(word_seed_matrix)\n",
    "    p_x_y_matrix = word_seed_matrix / count_all\n",
    "    p_x_vector = np.sum(word_seed_matrix, axis=1, keepdims=True) / count_all\n",
    "    p_y_vector = np.sum(word_seed_matrix, axis=0, keepdims=True) / count_all\n",
    "\n",
    "    with np.errstate(divide='ignore'):\n",
    "        pmi_matrix = np.log2(p_x_y_matrix / np.dot(p_x_vector, p_y_vector)) \n",
    "    # set PMI of independent pair to 0 \n",
    "    pmi_matrix = np.where(np.isneginf(pmi_matrix), 0, pmi_matrix) \n",
    "    # calc PPMI\n",
    "    ppmi_matrix = np.where(pmi_matrix < 0, 0, pmi_matrix)\n",
    "    # flip sign for negative vectors\n",
    "    ppmi_matrix[:,len(positive_seeds):] = - ppmi_matrix[:, len(positive_seeds):]\n",
    "    ppmi_score_matrix = ppmi_matrix\n",
    "\n",
    "    #generate positive/negative word list\n",
    "    score_vector = np.sum(ppmi_score_matrix, axis=1) / ppmi_score_matrix.shape[1]\n",
    "    pos_word_ids = np.nonzero(score_vector >= 0.3)[0]\n",
    "    neg_word_ids = np.nonzero(score_vector <= -0.3)[0]\n",
    "    pos_words = [id2word[id] for id in pos_word_ids]\n",
    "    neg_words = [id2word[id] for id in neg_word_ids]\n",
    "    \n",
    "    return pos_words, neg_words\n",
    "\n",
    "pos_words_ppmi, neg_words_ppmi = get_pos_neg_words(positive_seeds, negative_seeds)\n",
    "print '-------------------------Positive 5 examples--------------------------'\n",
    "print pos_words_ppmi[:5]\n",
    "print '-------------------------Negative 5 examples--------------------------'   \n",
    "print neg_words_ppmi[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "\n",
    "`Explanation of method`: When calculating PMI, we need to store each co-occurrence count and sum over 2 dimentions (1 over word in tweet, the other over seeds), so I prefer using matrix that simplify the calculation using numpy package. \n",
    "\n",
    "`Explanation of results`: All classification results from examples are weird. From my own opinion, the word ***smooth*** that in the negative list even conveys positive sense. We should pay more attention using our PPMI model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "e5d9e1b6fcc30a1a11d462200941ac3fe3c2be6ecf798ebb2ee1fad8"
   },
   "source": [
    "<b>Instructions</b>: Now you will test these automatically-produced lexicons against a manually-annotated set. There is a manually-built lexicon (the Hu and Liu lexicon) which is included with NLTK. It has a list of positive and negative words, which are accessed as below. First, investigate what percentage of the words in the manual lexicon are in each of the automatic lexicons, and then, only for those words which overlap and which are <em>not</em> in the seed set, evaluate the accuracy of with each of the automatic lexicons. Discuss the results, mentioning why you think the lexicon which won out did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "signature": "da057518000cd75efb08186052270ebecdd1ecded5559f6436e5c6c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "////////////////////////////////////////////////////////\n",
      "Ratios of automatic lexicons compared to Hu&Liu lexicon: \n",
      "SentiWorldNet: 0.150165016502(positive), 0.218256475047(negative)\n",
      "Seed generated (cosine similarity): 0.214336917563(positive), 0.218256475047(negative)\n",
      "Seed generated (PPMI): 0.0166056853363(positive), 0.0784917275875(negative)\n",
      "////////////////////////////////////////////////////////\n",
      "Accuracies of automatic lexicons (Take Hu & Liu lexicons as groud truth): \n",
      "SentiWorldNet, positive accuracy: 0.273273273273, negative accuracy: 0.28942408377\n",
      "Seed generated lexicon by using cosine similarity, positive accuracy: 0.299299299299, negative accuracy: 0.413612565445\n",
      "Seed generated lexicon by using PPMI, positive accuracy: 0.0295295295295, negative accuracy: 0.042722513089\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import opinion_lexicon\n",
    "positive_words = opinion_lexicon.positive()\n",
    "negative_words = opinion_lexicon.negative()\n",
    "\n",
    "pos_swn_set, neg_swn_set = set(pos_words_swn), set(neg_words_swn)\n",
    "pos_cosine_set, neg_cosine_set = set(pos_words_cos), set(neg_words_cos)\n",
    "pos_ppmi_set, neg_ppmi_set = set(pos_words_ppmi), set(neg_words_ppmi)\n",
    "\n",
    "pos_hl_set, neg_hl_set = set(positive_words), set(negative_words)\n",
    "\n",
    "swn_ratio = (len( pos_hl_set & pos_swn_set ) / float(len(pos_swn_set)), \n",
    "             len( neg_hl_set & neg_swn_set ) / float(len(neg_swn_set)))\n",
    "cosine_ratio = (len( pos_hl_set & pos_cosine_set ) / float(len(pos_cosine_set)),\n",
    "                len( neg_hl_set & neg_cosine_set ) / float(len(neg_cosine_set)))\n",
    "ppmi_ratio =  (len( pos_hl_set & pos_ppmi_set ) / float(len(pos_ppmi_set)),\n",
    "               len( neg_hl_set & neg_ppmi_set ) / float(len(neg_ppmi_set)))\n",
    "\n",
    "print '////////////////////////////////////////////////////////'\n",
    "print 'Ratios of automatic lexicons compared to Hu&Liu lexicon: '\n",
    "print 'SentiWorldNet: %s(positive), %s(negative)' % (swn_ratio[0], swn_ratio[1])\n",
    "print 'Seed generated (cosine similarity): %s(positive), %s(negative)' % (cosine_ratio[0], swn_ratio[1])\n",
    "print 'Seed generated (PPMI): %s(positive), %s(negative)' % (ppmi_ratio[0], ppmi_ratio[1])\n",
    "\n",
    "pos_seed_set, neg_seed_set = set(positive_seeds), set(negative_seeds)\n",
    "\n",
    "pos_dinominator = float(len(pos_hl_set - pos_seed_set))\n",
    "neg_dinominator = float(len(neg_hl_set - neg_seed_set))\n",
    "\n",
    "swn_accuracy = (len(pos_hl_set & pos_swn_set) / pos_dinominator,\n",
    "                len(neg_hl_set & neg_swn_set) / neg_dinominator)\n",
    "cosine_accuracy = (len(pos_hl_set & pos_cosine_set) / pos_dinominator,\n",
    "                len(neg_hl_set & neg_cosine_set) / neg_dinominator)\n",
    "ppmi_accuracy = (len(pos_hl_set & pos_ppmi_set) / pos_dinominator,\n",
    "                len(neg_hl_set & neg_ppmi_set) / neg_dinominator)\n",
    "\n",
    "print '////////////////////////////////////////////////////////'\n",
    "print 'Accuracies of automatic lexicons (Take Hu & Liu lexicons as groud truth): '\n",
    "print 'SentiWorldNet, positive accuracy: %s, negative accuracy: %s' % (swn_accuracy[0], swn_accuracy[1])\n",
    "print 'Seed generated lexicon by using cosine similarity, positive accuracy: %s, negative accuracy: %s' % \\\n",
    "        (cosine_accuracy[0], cosine_accuracy[1])\n",
    "print 'Seed generated lexicon by using PPMI, positive accuracy: %s, negative accuracy: %s' % \\\n",
    "        (ppmi_accuracy[0], ppmi_accuracy[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "\n",
    "If we treat Hu&Liu lexicon as groud truth, we can order the 3 automatic lexicons based on our self-defined accuracy as: CosineSim > SentiWorldNet > PPMI. However, in real cases, we can not roughly use this defination, because manually tagged lexicons are based on some specific human being who may have different backgroud, such as growing in different cultrue, only familiar with specific domain, the understanding of English, etc. \n",
    "\n",
    "From my own perspective, manual lexicons that genrated by domain users (samples created by statistical sampling methods if feasible) can be trusted. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "94017cf8e33a4c484ac0e569668a8e0a88dad8ad3562a192ab067595"
   },
   "source": [
    "<b>Instructions</b>: Now you will use the lexicons (both manual and automatic) for the main classification problem. Create a function which calculates a polarity score for a sentence based on a given lexicon (i.e. counting positive and negative words that appear in the tweet, and then returning +1 if there are more positive words, -1 if there are more negative words, and 0 otherwise). Then, use this to compare the results of the different lexicons (please convert them to sets!) on the task in the development set, i.e. the accuracy relative to the human-annotated labels. Do the results reflect the quality of the lexicon as indicated by the earlier analysis? How does it compare to the logistic regression classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "signature": "27208986ce7245a7acdf89740f413ab099ca30dcfbbe6878611e98dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentWorldNet Lexicon Absolute Loss: 1362.0, \tAccuracy: 0.404045926736\n",
      "CBOW-Cosine Lexicon Absolute Loss: 1195.0, \tAccuracy: 0.453799890651\n",
      "CBOW-PPMI Lexicon Absolute Loss: 1246.0, \tAccuracy: 0.448332422089\n",
      "Hu & Liu Lexicon Absolute Loss: 1286.0, \tAccuracy: 0.372334609076\n"
     ]
    }
   ],
   "source": [
    "def get_polarity_score(tweet, (pos_lexicon, neg_lexicon)):\n",
    "    pos_count, neg_count = 0, 0\n",
    "    for sentence in tweet:\n",
    "        for word in sentence:\n",
    "            if word in pos_lexicon:\n",
    "                pos_count += 1\n",
    "            if word in neg_lexicon:\n",
    "                neg_count += 1\n",
    "    if pos_count > neg_count:\n",
    "        return 1\n",
    "    elif pos_count < neg_count:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "# construct a tweet by lexicon (4 lexicons) matrix\n",
    "# follow the order: SentWorld Lexicon, CBOW-Cosine Lexicon, CBOW-PPMI Lexicon, Hu-Liu Lexicon\n",
    "tweet_lexicon_matrix = np.zeros((len(dev_tweets), 4))\n",
    "for row, tweet in enumerate(dev_tweets):\n",
    "    tweet_lexicon_matrix[row, 0] = get_polarity_score(tweet, (pos_swn_set, neg_swn_set))\n",
    "    tweet_lexicon_matrix[row, 1] = get_polarity_score(tweet, (pos_hl_set, neg_hl_set))\n",
    "    tweet_lexicon_matrix[row, 2] = get_polarity_score(tweet, (pos_cosine_set, neg_cosine_set))\n",
    "    tweet_lexicon_matrix[row, 3] = get_polarity_score(tweet, (pos_ppmi_set, neg_ppmi_set))\n",
    "\n",
    "# Evalute by absolute error and accuracy\n",
    "X, y = tweet_lexicon_matrix, dev_labels.reshape(len(dev_labels),1)\n",
    "# 4 elem vector\n",
    "abs_error = np.sum(np.fabs(X-y), axis=0)\n",
    "# 4 elem vector\n",
    "accuracy = np.sum(np.where((X-y)==0, 1, 0), axis=0) / float(len(y))\n",
    "    \n",
    "print 'SentWorldNet Lexicon Absolute Loss: %s, \\tAccuracy: %s' % (abs_error[0], accuracy[0])\n",
    "print 'CBOW-Cosine Lexicon Absolute Loss: %s, \\tAccuracy: %s' % (abs_error[1], accuracy[1])\n",
    "print 'CBOW-PPMI Lexicon Absolute Loss: %s, \\tAccuracy: %s' % (abs_error[2], accuracy[2])\n",
    "print 'Hu & Liu Lexicon Absolute Loss: %s, \\tAccuracy: %s' % (abs_error[3], accuracy[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "\n",
    "The result in this slot is different from the previous slot, especially PPMI accuracy. Accuracy of PPMI is way better than that in the previous slot. \n",
    "\n",
    "Compared to logistic regression, the result is bad, which means in our case, the lexicon method is beat by BOW statistical method.\n",
    "\n",
    "Additional explanation of evaluation:\n",
    "\n",
    "I use not only accuracy, but also absolute loss to evalute, because the accuracy defines error as binary value, it regards error \"-1 vs. 1\" as equal to \"0 vs. 1\", while absolute loss measures error more precisely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "9869efd812422d6c4a4684b89815a2393eed4e7b85b7611eed6c56aa"
   },
   "source": [
    "<b>Instructions</b>: Now you should investigate the effect of adding the polarity score (or scores) as a feature in your statistical classifier. You should create a new version of your convert_to_feature_dict function (with a different name) to include the extra feature (or features), do not modify the code in that earlier section directly. Retrain your best logistic regression classifier from the early tuning, test on the development set. Comment on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "signature": "38e04dbf26f86e48a64c7e03ac828ccf34335a9fcdbfdd9afd1f9326"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on development set: 0.504647348278\n"
     ]
    }
   ],
   "source": [
    "def construct_feature_dicts(tweets, lexicon_tuples, stopwords=set(), n=0):\n",
    "    ''' Lexicon tuples is a list of tuples - [(),()...], \n",
    "        each tuple is a pair of lexicons - (pos_lexicon, neg_lexicon),\n",
    "        each lexicon is a set'''\n",
    "    feature_dicts = convert_to_feature_dicts(tweets, stopwords, n)\n",
    "    \n",
    "    for tweet_idx, tweet in enumerate(tweets):\n",
    "        for pair_idx, pair in enumerate(lexicon_tuples):\n",
    "            feature_dicts[tweet_idx].setdefault('lexicon_%s' % pair_idx, \n",
    "                             get_polarity_score(tweet, pair))\n",
    "    return feature_dicts\n",
    "\n",
    "lexicon_tuples = [(pos_swn_set, neg_swn_set),\n",
    "                  (pos_cosine_set, neg_cosine_set),\n",
    "                  (pos_ppmi_set, neg_ppmi_set),\n",
    "                  (pos_hl_set, neg_hl_set)\n",
    "                 ]\n",
    "\n",
    "train_feature_dicts = construct_feature_dicts(train_tweets,\n",
    "                                  lexicon_tuples,\n",
    "                                  stopwords,\n",
    "                                  1)\n",
    "dev_feature_dicts = construct_feature_dicts(dev_tweets,lexicon_tuples)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# re-create train set and dev set\n",
    "vectorizer = DictVectorizer()\n",
    "train_data = vectorizer.fit_transform(train_feature_dicts)\n",
    "dev_data = vectorizer.transform(dev_feature_dicts)\n",
    "\n",
    "best_clf = LogisticRegression(solver='lbfgs',\n",
    "                              C=0.1,\n",
    "                              max_iter=100,\n",
    "                              multi_class='multinomial')\n",
    "best_clf.fit(train_data, train_labels)\n",
    "preds = best_clf.predict(dev_data)\n",
    "print 'Accuracy on development set: %s' % accuracy_score(dev_labels, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "\n",
    "Compared to logistic regression without lexicon feature (accuracy 50.02%), the result here (accuracy 50.46%) improves a little, but not impressive. Beyond the result in this slot, I also explored all combinations of lexicons, if we number SentiWorldNet case, CosineSim case, PPMI case and Hu&Liu case as 1, 2, 3, 4, We got the accuracy table as:\n",
    "\n",
    "\n",
    "|combination |1     |2     |3     |  4   |  1+2 |  1+3 |  1+4 |  2+3 |  2+4 |  3+4 | 1+2+3| 1+2+4| 2+3+4|1+2+3+4|\n",
    "| ---------- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |\n",
    "|**accuracy**|0.4908|0.5002|0.5019|0.5084|0.5030|0.4986|0.5068|0.5019|0.5068|0.5079|0.5030|0.5057|0.5057|0.5046|\n",
    "\n",
    "So, the best combination on development set is 3+4 (PPMI + Hu&Liu) lexicons. As the difference between best combination and all lexicons' combination is so small (0.003%) that shall not be treated as structural risk, we still use all lexicons to avoid bias.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "4033087332032873cbec44070a3024f2d7b4190c5c214fcde203f999"
   },
   "source": [
    "## Error analysis and improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "9e608687d8b2351d1bb87ba2dcc7ab319757f264c0b5e1b3a791180c"
   },
   "source": [
    "<b>Instructions</b>: Using your best logistic regression classifier so far, first write a function to identify errors your classifier is making where the probability of the predicted class and the actual class are fairly close (less than 0.2); you're looking for cases which you have a good chance of getting right with a small improvement. For this, do an 80/20 split of  the training dataset (that is, train on 80% of the data, test on 20%); do not look at examples from the development set or the test set. You should print out the tweet, the correct class, the predicted class, and the probabilities. Don't print all the errors, just use random.sample to select 30 from the full set. Look for general patterns in the errors, and propose a reasonable improvement to your classifier that you think might help with a problem that you are seeing. It could involve, for instance, better preprocessing, the addition of new features, some kind of feature selection, better lexicons or better use of the lexicons, or even a post-processing step. It should not require additional data, unless it involves a small set of words that you can hardcode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "signature": "a0595cfd489a8cb3349a1ea3437519d1410646d524baf63e078cc021"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: [[u'are', u'you', u'really', u'saying', u'that', u'fps', u'russia', u'plays', u'with', u'airsoft', u'guns', u'...', u'c', u\"'\", u'mon', u'he', u'doesn', u\"'\", u't', u'need', u'that', u'he', u\"'\", u's', u'got', u'the', u'real', u'shit']]\n",
      "True class: 1\n",
      "Predicted class: -1\n",
      "Probability: [ 0.91130264  0.06470451  0.02399285]\n",
      "\n",
      "Tweet: [[u'red', u'cross', u'halts', u'most', u'pakistan', u'aid', u'after', u'beheading', u':', u'the', u'international', u'committee', u'of', u'the', u'red', u'cross', u'said', u'on', u'tuesday', u'...']]\n",
      "True class: -1\n",
      "Predicted class: 0\n",
      "Probability: [ 0.03676162  0.88830328  0.0749351 ]\n",
      "\n",
      "Tweet: [[u'is', u'anybody', u'going', u'to', u'the', u'strawberry', u'festival', u',', u'in', u'leicestershire', u'...', u'i', u'will', u'be', u'painting', u'there', u'live', u'on', u'the', u'friday', u'and', u'saturday']]\n",
      "True class: 1\n",
      "Predicted class: 0\n",
      "Probability: [ 0.02109252  0.86136784  0.11753964]\n",
      "\n",
      "Tweet: [[u'tumblr', u'keeps', u'eating', u'my', u'messages', u'.'], [u'something', u'came', u'up', u'tho', u'sobs', u'.'], [u';', u'__', u';', u'i', u\"'\", u'll', u'tell', u'everything', u'tomorrow', u'/', u'tonight', u'.'], [u'i', u'love', u'you', u'lots', u';;']]\n",
      "True class: -1\n",
      "Predicted class: 1\n",
      "Probability: [ 0.0118234   0.09804246  0.89013414]\n",
      "\n",
      "Tweet: [[u'please', u'join', u'lpg', u'and', u'good', u'start', u'genetics', u'for', u'a', u'legal', u'town', u'hall', u'cocktail', u'reception', u'monday', u'@', u'7', u'pm', u'at', u'the', u'marriott', u'marquis', u'mission', u'hills', u'room', u'.']]\n",
      "True class: 0\n",
      "Predicted class: 1\n",
      "Probability: [ 0.01131091  0.13001062  0.85867847]\n",
      "\n",
      "Tweet: [[u'lsu', u'needs', u'to', u'do', u'one', u'thing', u'to', u'beat', u'bama', u'.'], [u'run', u'!'], [u'and', u'stop', u'the', u'run', u'!'], [u'win', u'it', u'in', u'the', u'4th', u'quarter', u',', u'geaux', u'tigers', u'!'], [u'!']]\n",
      "True class: 0\n",
      "Predicted class: 1\n",
      "Probability: [ 0.0196016  0.0840821  0.8963163]\n",
      "\n",
      "Tweet: [[u'bjp', u'doing', u'drama', u'to', u'hide', u'facts', u',', u'says', u'minister', u'-', u'the', u'government', u'tuesday', u'accused', u'the', u'bharatiya', u'janata', u'party', u'(', u'bjp', u')', u'of', u'...']]\n",
      "True class: -1\n",
      "Predicted class: 0\n",
      "Probability: [ 0.03396915  0.87644594  0.08958492]\n",
      "\n",
      "Tweet: [[u'ate', u'is', u'giving', u'u', u'l', u'o', u'l', u'the', u'business', u'...', u'long', u'pass', u'play', u'from', u'aplin', u'to', u'jones', u'-', u'red', u'wolves', u'up', u'20', u'-', u'0', u'2nd', u'q', u'sun', u'belt']]\n",
      "True class: 1\n",
      "Predicted class: 0\n",
      "Probability: [ 0.01784709  0.87709071  0.1050622 ]\n",
      "\n",
      "Tweet: [[u'the', u'international', u'committee', u'of', u'the', u'red', u'cross', u'on', u'friday', u'warned', u'that', u'three', u'palestinian', u'hunger', u'strikers', u'were', u'at', u'risk', u'...']]\n",
      "True class: -1\n",
      "Predicted class: 0\n",
      "Probability: [ 0.15066532  0.80552975  0.04380493]\n",
      "\n",
      "Tweet: [[u'alright', u'.'], [u'g', u\"'\", u'night', u'!'], [u'tomorrow', u'i', u'will', u'be', u'in', u'california', u'.'], [u'the', u'next', u'day', u'i', u'will', u'see', u'bigbang', u'with', u'my', u'own', u'eyes', u'.'], [u'no', u'pressure', u'on', u'the', u'sleeping', u'now', u'.'], [u':', u'p']]\n",
      "True class: 0\n",
      "Predicted class: 1\n",
      "Probability: [ 0.02997965  0.16781444  0.80220591]\n",
      "\n",
      "Tweet: [[u'i', u'am', u'manipulative', u':', u'knightley', u':', u'los', u'angeles', u',', u'nov', u'2', u':', u'actress', u'keira', u'knightley', u'says', u'although', u'she', u'does', u'not', u'like', u'some', u't', u'...']]\n",
      "True class: -1\n",
      "Predicted class: 0\n",
      "Probability: [ 0.0685305   0.85080022  0.08066928]\n",
      "\n",
      "Tweet: [[u'd', u't', u'make', u'sure', u'you', u'find', u'your', u'self', u'at', u'the', u'\"', u's', u'.', u'e', u'.', u'c', u'\"', u'this', u'thursday', u'for', u'$', u'3', u'thursdays', u'!!'], [u'$', u'3', u'w', u'/', u'college', u'id', u'$', u'5', u'w', u'/', u'reg', u'id', u'til', u'11pm', u'!!!'], [u'party']]\n",
      "True class: 1\n",
      "Predicted class: 0\n",
      "Probability: [ 0.01324436  0.84403048  0.14272517]\n",
      "\n",
      "Tweet: [[u'my', u'4th', u'of', u'july', u'song', u'is', u\"'\", u'ito', u'ang', u'gusto', u'ko', u\"'\", u'by', u'francis', u'm', u'.', u'(', u'best', u'for', u'independence', u'day', u'here', u'in', u'the', u'ph', u')', u'galaxy', u'play', u'trackofthe', u'day']]\n",
      "True class: 0\n",
      "Predicted class: 1\n",
      "Probability: [ 0.01459091  0.12593424  0.85947485]\n",
      "\n",
      "Tweet: [[u'betting', u'on', u'the', u'honey', u'badger', u':', u'will', u'tyrann', u'mathieu', u'make', u'it', u'to', u'the', u'nfl', u'?'], [u':', u'this', u'aug', u'.', u'2', u',', u'2012', u'file', u'photo', u'shows', u'lsu', u'cor', u'...']]\n",
      "True class: 1\n",
      "Predicted class: 0\n",
      "Probability: [ 0.0436232   0.85544512  0.10093169]\n",
      "\n",
      "Tweet: [[u'physics', u'professor', u'just', u'extended', u'our', u'assignment', u'from', u'11', u':', u'30', u'tonight', u'to', u'11', u':', u'30', u'tomorrow', u'...', u'i', u'guess', u'he', u'isn', u\"'\", u't', u'the', u'only', u'one', u'drinking', u'.'], [u'prof', u'get', u'crunk']]\n",
      "True class: 1\n",
      "Predicted class: 0\n",
      "Probability: [ 0.06077349  0.84495237  0.09427414]\n",
      "\n",
      "Tweet: [[u'49ersparadise', u'hop', u':', u'pick', u'your', u'poison', u':', u'eliminating', u'vernon', u'davis', u'comes', u'with', u'a', u'price', u'[', u'bee', u'-', u'blog', u']:', u'mon', u',', u'29', u'oct', u'2012', u'8', u':...']]\n",
      "True class: -1\n",
      "Predicted class: 0\n",
      "Probability: [ 0.02966211  0.92704092  0.04329697]\n",
      "\n",
      "Tweet: [[u'well', u'geaux', u'tigers', u':)', u'and', u'yes', u'it', u'is', u'and', u'then', u'three', u'days', u'after', u'that', u'is', u'.....', u'nov', u'6', u'which', u'means', u'election', u'day', u'!'], [u'romney', u'ryan', u'!']]\n",
      "True class: 0\n",
      "Predicted class: 1\n",
      "Probability: [ 0.01589168  0.08589869  0.89820963]\n",
      "\n",
      "Tweet: [[u'rt', u'be', u'sure', u'to', u'catch', u'former', u'owl', u'football', u'player', u'lestar', u'jean', u'on', u'espn', u\"'\", u's', u'monday', u'night', u'football', u'tonight', u'at', u'8', u'p', u'.', u'm', u'.', u'with', u'hous', u'...']]\n",
      "True class: 1\n",
      "Predicted class: 0\n",
      "Probability: [ 0.03524881  0.86734327  0.09740792]\n",
      "\n",
      "Tweet: [[u'live', u':', u'sunderland', u'v', u'newcastle', u':', u'it', u'may', u'not', u'have', u'the', u'glamour', u'of', u'el', u'clasico', u'or', u'the', u'title', u'implications', u'of', u'the', u'manches', u'...']]\n",
      "True class: -1\n",
      "Predicted class: 0\n",
      "Probability: [ 0.10813539  0.80072724  0.09113737]\n",
      "\n",
      "Tweet: [[u'kirk', u\"'\", u's', u'bike', u'shop', u':', u'support', u'the', u'businesses', u'you', u'love', u'and', u'shop', u'small', u'this', u'november', u'24th', u'.']]\n",
      "True class: 0\n",
      "Predicted class: 1\n",
      "Probability: [ 0.01986425  0.1427547   0.83738105]\n",
      "\n",
      "Tweet: [[u'face', u'like', u'miss', u'america', u'...', u'body', u'like', u'a', u'stripper', u'...', u'fuck', u'me', u'like', u'a', u'pornstar', u'...', u'n', u'handle', u'business', u'like', u'the', u'1st', u'lady', u'...', u'yea', u'i', u'need', u'that', u'...$$$']]\n",
      "True class: 1\n",
      "Predicted class: -1\n",
      "Probability: [ 0.90737184  0.0662296   0.02639855]\n",
      "\n",
      "Tweet: [[u'a', u'never', u'-', u'ending', u'gold', u'rush', u'in', u'china', u':', u'china', u',', u'the', u'largest', u'producer', u'of', u'gold', u',', u'may', u'also', u'become', u'the', u'biggest', u'consumer', u'of', u'...']]\n",
      "True class: 1\n",
      "Predicted class: 0\n",
      "Probability: [ 0.05317666  0.81321137  0.13361197]\n",
      "\n",
      "Tweet: [[u'hahahaha', u'!'], [u'busted', u'.'], [u'i', u'just', u'met', u'hib', u'today', u'hahaha', u'!'], [u'ok', u'on', u'monday', u'kiter', u'buke', u'together', u'gether', u'!'], [u':', u'd', u'free', u'labour', u'included', u'tak', u'?'], [u'o', u'.', u'o']]\n",
      "True class: 0\n",
      "Predicted class: 1\n",
      "Probability: [ 0.00452838  0.01705473  0.97841689]\n",
      "\n",
      "Tweet: [[u'bjp', u'members', u'walk', u'out', u'from', u'jpc', u'meeting', u'on', u'2g', u':', u'bharatiya', u'janata', u'party', u'(', u'bjp', u')', u'members', u'on', u'tuesday', u'walked', u'out', u'of', u'the', u'p', u'...']]\n",
      "True class: -1\n",
      "Predicted class: 0\n",
      "Probability: [ 0.03142203  0.84286597  0.12571199]\n",
      "\n",
      "Tweet: [[u'gold', u'edges', u'down', u'ahead', u'of', u'us', u'jobs', u'data', u':', u'singapore', u'(', u'reuters', u')', u'-', u'gold', u'edged', u'lower', u'on', u'friday', u',', u'with', u'investors', u'waiting', u'for', u'...']]\n",
      "True class: -1\n",
      "Predicted class: 0\n",
      "Probability: [ 0.01958528  0.86718164  0.11323308]\n",
      "\n",
      "Tweet: [[u'in', u'the', u'sports', u'world', u'report', u':', u'steelers', u'can', u\"'\", u't', u'find', u'hotel', u',', u'to', u'travel', u'sun', u'.'], [u':', u'the', u'pittsburgh', u'steelers', u'will', u'fly', u'to', u'new', u'y', u'...']]\n",
      "True class: -1\n",
      "Predicted class: 0\n",
      "Probability: [ 0.0422384   0.84042302  0.11733859]\n",
      "\n",
      "Tweet: [[u'i', u'doubt', u'it', u'.'], [u'but', u'we', u'may', u'just', u'get', u'to', u'see', u'in', u'a', u'bcs', u'bowl', u'in', u'january', u'.'], [u'as', u'for', u'kstate', u',', u'they', u\"'\", u're', u'good', u',', u'just', u'not', u'sec', u'good', u'.']]\n",
      "True class: 0\n",
      "Predicted class: 1\n",
      "Probability: [ 0.04606982  0.04514964  0.90878054]\n",
      "\n",
      "Tweet: [[u'i', u'heard', u'may', u'queen', u'is', u'good', u',', u'nice', u'guy', u'but', u'has', u'song', u'joongki', u'in', u'it', u'^^~', u'i', u'have', u'no', u'interested', u'in', u'drama', u'these', u'days', u'unnie', u'=.=']]\n",
      "True class: -1\n",
      "Predicted class: 1\n",
      "Probability: [ 0.01067132  0.07879097  0.91053771]\n",
      "\n",
      "Tweet: [[u'ive', u'got', u'a', u'maths', u'exam', u'tomorrow', u'.'], [u'can', u'you', u'wish', u'me', u'a', u'luck', u'?'], [u'no', u'?'], [u'okay', u'thanks', u'.'], [u'i', u'wont', u'pass', u'this', u'exam', u'.', u'4']]\n",
      "True class: -1\n",
      "Predicted class: 1\n",
      "Probability: [ 0.0439187  0.1109513  0.84513  ]\n",
      "\n",
      "Tweet: [[u'd', u't', u'make', u'sure', u'you', u'find', u'your', u'self', u'at', u'the', u'\"', u's', u'.', u'e', u'.', u'c', u'\"', u'this', u'thursday', u'for', u'$', u'3', u'thursdays', u'!!!'], [u'$', u'3', u'w', u'/', u'college', u'id', u'$', u'5', u'w', u'/', u'reg', u'.'], [u'id', u'til', u'11pm', u'!!!'], [u'party']]\n",
      "True class: 1\n",
      "Predicted class: 0\n",
      "Probability: [ 0.01060883  0.86796867  0.1214225 ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "def get_significant_errors(clf, data, labels, tweets):\n",
    "    \n",
    "    errors = []\n",
    "    # get train set and test set\n",
    "    split_size = int(0.8*data.shape[0])\n",
    "    X_train, y_train = data[:split_size,:], labels[:split_size]\n",
    "    X_test, y_test = data[split_size:,:], labels[split_size:]\n",
    "    test_tweets = tweets[split_size:]\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    preds, pred_probas = clf.predict(X_test), clf.predict_proba(X_test)\n",
    "    \n",
    "    for idx, truth in enumerate(y_test):\n",
    "        # probability fairly close error\n",
    "        if preds[idx] != truth and max(pred_probas[idx]) > 0.8:\n",
    "            errors.append([test_tweets[idx], truth, preds[idx], pred_probas[idx]])\n",
    "            \n",
    "    return errors\n",
    "    \n",
    "errors = get_significant_errors(best_clf, train_data, train_labels, tweets)\n",
    "sample_errors = random.sample(errors, 30)\n",
    "for error in sample_errors:\n",
    "    print 'Tweet: %s' % error[0]\n",
    "    print 'True class: %s' % error[1]\n",
    "    print 'Predicted class: %s' % error[2]\n",
    "    print 'Probability: %s' % error[3]\n",
    "    print "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**\n",
    "\n",
    "***Phenomanon***\n",
    "1. 16/30 neutral error predictions ('0') , 8/30 neutral truth that is almost uniformly distributed. \n",
    "2. The ratio of completely dead error (truth 1, predicted -1 or verse visa) based on polarity is 3/30\n",
    "\n",
    "***Corresponding (to phenomanon) Methods***\n",
    "1. Add contribution of less frequent words (remove the corrsponding preprocessing option) (Improvement Method 1)\n",
    "2. Re-construct lexicons to add the lexicon contribution to final predictions\n",
    "    1. Union lexicons and boostrap the union to generate more lexicons with bigger size. (Improvement method 2)\n",
    "2. Use sentence based BOW instead of tweet based BOW. Since each tweet has 1+ sentences, this method can increase the training set size:\n",
    "    1. Re-define feature dicts construction function. Maintain tweet to sentence mapping table. (Improvement method 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "94f0903c93aae6b346f8b432150307a007e3c8875a0273c645c91d07"
   },
   "source": [
    "<b>Instructions</b>: Now implement that improvement, and then investigate its effect <em>in the development data</em>. Obviously, different improvements may involve different amounts of effort; if your improvement is fairly simple, we expect that you will do a more in-depth analysis, testing possible variations. You can also do multiple related improvements. Students who put extra effort into this may get few extra points that can offset any mistakes on other parts of the assignment, though we do not recommend you spend extra time on this before the other parts of the assignment are complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "signature": "6447472d8dba639dfa84b3b9af3decc28a33cd3190932e58b7c14107"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on development set (Improvement Method 1): 0.509021323127\n"
     ]
    }
   ],
   "source": [
    "# improvement method 1: remove polarity score on stopwords\n",
    "def get_dict_polarity_score(dic, (pos, neg)):\n",
    "    ''' For training set, pose score on every feature dictionary\n",
    "        which remove the score on stopwords'''\n",
    "    pos_count, neg_count = 0, 0\n",
    "    for word in dic.keys():\n",
    "        if word in pos:\n",
    "            pos_count += 1\n",
    "        elif word in neg:\n",
    "            neg_count += 1\n",
    "    if pos_count > neg_count:\n",
    "        return 1\n",
    "    elif pos_count < neg_count:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def add_lex_to_featured(feature_dicts, lexicon_tuples):\n",
    "    ''' same as in function construct_feature_dicts(), only calculate score by \n",
    "        function get_dict_polarity_score() '''\n",
    "    for dic_id, dic in enumerate(feature_dicts):\n",
    "        for lex_id, pair in enumerate(lexicon_tuples):\n",
    "            feature_dicts[dic_id].setdefault('lexicon_%s' % \\\n",
    "                        lex_id, get_dict_polarity_score(dic, pair))\n",
    "    return feature_dicts\n",
    "\n",
    "# set n = 0, in order to tag score on every token except stopwords \n",
    "train_featured = convert_to_feature_dicts(train_tweets, stopwords, 0)\n",
    "train_featured = add_lex_to_featured(train_featured, lexicon_tuples)\n",
    "\n",
    "dev_featured = add_lex_to_featured(convert_to_feature_dicts(dev_tweets), lexicon_tuples)\n",
    "\n",
    "vectorizer = DictVectorizer()\n",
    "train_data = vectorizer.fit_transform(train_featured)\n",
    "dev_data = vectorizer.transform(dev_featured)\n",
    "\n",
    "best_clf = LogisticRegression(solver='lbfgs',\n",
    "                              C=0.1,\n",
    "                              max_iter=100,\n",
    "                              multi_class='multinomial')\n",
    "best_clf.fit(train_data, train_labels)\n",
    "preds = best_clf.predict(dev_data)\n",
    "print 'Accuracy on development set (Improvement Method 1): %s' % accuracy_score(dev_labels, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original lexicons accuracy: 0.504647348278\n",
      "Bootstrapped lexicons accuracy (Improvement Method 2): 0.501366867141\n"
     ]
    }
   ],
   "source": [
    "# improvement method 2: bootstrap resample lexicons\n",
    "import random\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "def bootstrap_lexicons(lexicon_tuples, ratio=0.5, number=10):\n",
    "    '''Union the 4 lexicons and bootstrap @number samples.\n",
    "       Each sample has the size of @ratio*len(union_lexicon)'''\n",
    "    # construct union lexicon\n",
    "    pos_union_lex, neg_union_lex = set(), set()\n",
    "    for pos, neg in lexicon_tuples:\n",
    "        pos_union_lex |= pos\n",
    "        neg_union_lex |= neg\n",
    "    # remove intersected elements\n",
    "    intersect_set = pos_union_lex & neg_union_lex\n",
    "    #print len(intersect_set), len(pos_union_lex), len(neg_union_lex)\n",
    "    #sys.exit()\n",
    "    pos_union_lex -= intersect_set\n",
    "    neg_union_lex -= intersect_set\n",
    "    \n",
    "    # bootstrap lexicons\n",
    "    return [(set(random.sample(pos_union_lex, int(ratio*len(pos_union_lex)))), \\\n",
    "             set(random.sample(neg_union_lex, int(ratio*len(neg_union_lex))))) \\\n",
    "            for i in xrange(number)]\n",
    "\n",
    "def get_dev_accuracy(lexs):\n",
    "    train_feature_dicts = construct_feature_dicts(train_tweets,\n",
    "                                  lexs,\n",
    "                                  stopwords,\n",
    "                                  1)\n",
    "    dev_feature_dicts = construct_feature_dicts(dev_tweets,lexs)\n",
    "\n",
    "    # re-create train set and dev set\n",
    "    vectorizer = DictVectorizer()\n",
    "    train_data = vectorizer.fit_transform(train_feature_dicts)\n",
    "    dev_data = vectorizer.transform(dev_feature_dicts)\n",
    "\n",
    "    clf = LogisticRegression(solver='lbfgs',\n",
    "                                  C=0.1,\n",
    "                                  max_iter=100,\n",
    "                                  multi_class='multinomial')\n",
    "    clf.fit(train_data, train_labels)\n",
    "    preds = clf.predict(dev_data)\n",
    "    return accuracy_score(dev_labels, preds)\n",
    "\n",
    "# original lexicons' accuracy\n",
    "print 'Original lexicons accuracy: %s' % get_dev_accuracy(lexicon_tuples)\n",
    "# accuracy on bootstrap lexicons\n",
    "lexs_bootstrap = bootstrap_lexicons(lexicon_tuples, 0.7, 4)\n",
    "print 'Bootstrapped lexicons accuracy (Improvement Method 2): %s' % get_dev_accuracy(lexs_bootstrap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on development set (Improvement Method 3): 0.518316019683\n"
     ]
    }
   ],
   "source": [
    "# improvement method 3: use sentence instead of tweet\n",
    "def get_sent_feature_dicts(tweets, stopwords=set(), n=0):\n",
    "    ''' Function like convert_to_feature_dicts(), only every dictionary \n",
    "        denotes a sentence in a tweet instead of that tweet.\n",
    "        Return also an list that map tweet to sentence'''\n",
    "    feature_dicts = []\n",
    "    # record the occurance of tokens based on entire training set\n",
    "    global_feature_dict = {}\n",
    "    # map[tweet_id] is [index in feature_dicts]\n",
    "    tweet_sent_map = []\n",
    "    for tweet_idx, tweet in enumerate(tweets):\n",
    "        tweet_sent_map.append([])\n",
    "        for sentence in tweet:\n",
    "            feature_dict = {}\n",
    "            for token in sentence:\n",
    "                # skip stop words\n",
    "                if token in stopwords:\n",
    "                    continue \n",
    "                # add token into dict\n",
    "                if token not in feature_dict:\n",
    "                    feature_dict.setdefault(token, 1)\n",
    "                else:\n",
    "                    feature_dict[token] = feature_dict[token] + 1\n",
    "                # add token to global dictionary preparing for removing less frequency tokens\n",
    "                if token not in global_feature_dict:\n",
    "                    global_feature_dict.setdefault(token, 1)\n",
    "                else:\n",
    "                    global_feature_dict[token] = global_feature_dict[token] + 1\n",
    "            feature_dicts.append(feature_dict)\n",
    "            # sentence index in feature dicts is len(feature_dicts) -1 \n",
    "            tweet_sent_map[tweet_idx].append(len(feature_dicts)-1)\n",
    "                                            \n",
    "    \n",
    "    if n <= 0:\n",
    "        return feature_dicts, tweet_sent_map\n",
    "    \n",
    "    # remove feature that occur less than n times\n",
    "    for idx, dic in enumerate(feature_dicts):\n",
    "        for feature in dic.keys():\n",
    "            if global_feature_dict[feature] <= n:\n",
    "                feature_dicts[idx].pop(feature)\n",
    "             \n",
    "    return feature_dicts, tweet_sent_map\n",
    "\n",
    "    \n",
    "def get_sent_dicts_label(tweets, labels, lexicon_tuples, stopwords=set(), n=0, trainset=True):\n",
    "    ''' '''\n",
    "    if trainset:\n",
    "        sent_feature_dicts, tweet_sent_map = get_sent_feature_dicts(tweets, stopwords, n)\n",
    "    else:\n",
    "        sent_feature_dicts, tweet_sent_map = get_sent_feature_dicts(tweets)\n",
    "        \n",
    "    # As stopwords merely have polarity sense, we directly use add_lex_to_featured(), \n",
    "    # difference is only performing on sentence based feature dictionary\n",
    "    feature_dicts = add_lex_to_featured(sent_feature_dicts, lexicon_tuples)\n",
    "    sent_labels = []\n",
    "    \n",
    "    for idx, label in enumerate(labels):\n",
    "        # label and tweet are mapped in same order, they share the same idx\n",
    "        for sent_idx in tweet_sent_map[idx]:\n",
    "            sent_labels.append(label)\n",
    "            \n",
    "    return sent_feature_dicts, sent_labels, tweet_sent_map\n",
    "        \n",
    "    \n",
    "def get_tweet_label_from_sents(tweet_idx, preds, tweet_sent_map):\n",
    "    ''' Map predicted sentence labels back to tweet.\n",
    "        More positive sent labels in a tweet yield \n",
    "        positive tweet label, etc.'''\n",
    "    sents_ids = tweet_sent_map[tweet_idx]\n",
    "    label = 0\n",
    "    for idx in sents_ids:\n",
    "        label += preds[idx]\n",
    "    if label > 0:\n",
    "        return 1\n",
    "    elif label < 0:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "            \n",
    "train_sent_dicts, train_sent_labels, _ = get_sent_dicts_label(train_tweets, \n",
    "                                                              train_labels, \n",
    "                                                              lexicon_tuples,\n",
    "                                                              stopwords=stopwords,\n",
    "                                                              n=1)\n",
    "dev_sent_dicts, _, dev_tweet_sent_map = get_sent_dicts_label(dev_tweets, \n",
    "                                                             dev_labels, \n",
    "                                                             lexicon_tuples, \n",
    "                                                             trainset=False)\n",
    "\n",
    "vectorizer = DictVectorizer()\n",
    "train_sent_x = vectorizer.fit_transform(train_sent_dicts)\n",
    "dev_sent_x = vectorizer.transform(dev_sent_dicts)\n",
    "\n",
    "best_clf = LogisticRegression(solver='lbfgs',\n",
    "                                  C=0.1,\n",
    "                                  max_iter=100,\n",
    "                                  multi_class='multinomial')\n",
    "best_clf.fit(train_sent_x, train_sent_labels)\n",
    "preds = best_clf.predict(dev_sent_x)\n",
    "dev_tweet_preds = []\n",
    "for idx in xrange(len(dev_labels)):\n",
    "    dev_tweet_preds.append(get_tweet_label_from_sents(idx, preds, dev_tweet_sent_map))\n",
    "print 'Accuracy on development set (Improvement Method 3): %s' % accuracy_score(dev_labels, dev_tweet_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "5e3b7d3c5af2a6e900c64a3747dccdb0303d43659ef6b0b9ea89318f"
   },
   "source": [
    "## Final testing and discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "188e1a449f8822871c20d6951caffc1137e606feda9879e270e55325"
   },
   "source": [
    "<b>Instructions</b>: When the final test set has been released, you should start with your best classifier from your work up to this point, and do a final test of all major options, including at least one from each of the first four sections of the assignment (that is, at least one preprocessing option, at least one tuning parameter/classifier type, at least one lexicon, and your improvement), in this new dataset. You don't need to explore every possible combination (in fact, you shouldn't), but you should make a convincing case that you have probably found the best combination given the possibilities you have implemented. It's okay if you find discrepancies between the best classifier on the test set and development set, just be sure to mention them. In a final discussion (which should be at least 500 words), you should include at least one bar graph of accuracy across various options, and at least one table which reports precision, recall, and F-score for each label as well as the macroaveraged F-score (all figures and tables should be generated inline by your code, using matplotlib). Please conclude your discussion by discussing what you have learned, and mentioning any other ideas for improving performance of this system that you may have.\n",
    "\n",
    "Note that you may have to direct matplotlib to display the figures inline, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "test_tweets, test_labels = preprocess_file('test.json')\n",
    "predictions_, truths_ = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "signature": "5684fa2c3bbcdc1f42077375d86d01918b9b5e8ff43b8280e3aabe15"
   },
   "outputs": [],
   "source": [
    "# No option, the original method without any extra settings.\n",
    "train_feature_dicts = convert_to_feature_dicts(train_tweets)\n",
    "test_feature_dicts = convert_to_feature_dicts(test_tweets)\n",
    "\n",
    "vectorizer = DictVectorizer()\n",
    "train_data = vectorizer.fit_transform(train_feature_dicts)\n",
    "test_data = vectorizer.transform(test_feature_dicts)\n",
    "\n",
    "clf = LogisticRegression(solver='lbfgs',\n",
    "                        multi_class='multinomial')\n",
    "clf.fit(train_data, train_labels)\n",
    "pred = clf.predict(test_data)\n",
    "predictions_.append(pred)\n",
    "truths_.append(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# preprocessing option: remove stop words from building feature dicts\n",
    "train_feature_dicts = convert_to_feature_dicts(train_tweets, stopwords)\n",
    "test_feature_dicts = convert_to_feature_dicts(test_tweets,)\n",
    "\n",
    "vectorizer = DictVectorizer()\n",
    "train_data = vectorizer.fit_transform(train_feature_dicts)\n",
    "test_data = vectorizer.transform(test_feature_dicts)\n",
    "\n",
    "clf = LogisticRegression(solver='lbfgs',\n",
    "                        #C=0.1,\n",
    "                        #max_iter=100,\n",
    "                        multi_class='multinomial')\n",
    "clf.fit(train_data, train_labels)\n",
    "pred = clf.predict(test_data)\n",
    "predictions_.append(pred)\n",
    "truths_.append(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# best tuning option\n",
    "train_feature_dicts = convert_to_feature_dicts(train_tweets)\n",
    "test_feature_dicts = convert_to_feature_dicts(test_tweets)\n",
    "\n",
    "vectorizer = DictVectorizer()\n",
    "train_data = vectorizer.fit_transform(train_feature_dicts)\n",
    "test_data = vectorizer.transform(test_feature_dicts)\n",
    "\n",
    "clf = LogisticRegression(solver='lbfgs',\n",
    "                                  C=0.1,\n",
    "                                  max_iter=100,\n",
    "                                  multi_class='multinomial')\n",
    "clf.fit(train_data, train_labels)\n",
    "pred = clf.predict(test_data)\n",
    "predictions_.append(pred)\n",
    "truths_.append(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lexicon options\n",
    "train_feature_dicts = construct_feature_dicts(train_tweets, lexicon_tuples)\n",
    "test_feature_dicts = construct_feature_dicts(test_tweets, lexicon_tuples)\n",
    "\n",
    "vectorizer = DictVectorizer()\n",
    "train_data = vectorizer.fit_transform(train_feature_dicts)\n",
    "test_data = vectorizer.transform(test_feature_dicts)\n",
    "\n",
    "clf = LogisticRegression(solver='lbfgs',\n",
    "                        multi_class='multinomial')\n",
    "clf.fit(train_data, train_labels)\n",
    "pred = clf.predict(test_data)\n",
    "predictions_.append(pred)\n",
    "truths_.append(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use improvement method 1 and 3 to build feature dicts (sentence based feature dicts)\n",
    "train_sent_dicts, train_sent_labels, _ = get_sent_dicts_label(train_tweets, \n",
    "                                                              train_labels, \n",
    "                                                              [],)\n",
    "test_sent_dicts, _, test_tweet_sent_map = get_sent_dicts_label(test_tweets,\n",
    "                                                              test_labels,\n",
    "                                                              [],\n",
    "                                                              trainset=False)\n",
    "vectorizor = DictVectorizer()\n",
    "train_sent_x = vectorizer.fit_transform(train_sent_dicts)\n",
    "test_sent_x = vectorizer.transform(test_sent_dicts)\n",
    "\n",
    "clf = LogisticRegression(solver='lbfgs',\n",
    "                                  #C=0.1,\n",
    "                                  #max_iter=100,\n",
    "                                  multi_class='multinomial')\n",
    "\n",
    "clf.fit(train_sent_x, train_sent_labels)\n",
    "preds = clf.predict(test_sent_x)\n",
    "test_tweet_preds = []\n",
    "for idx in xrange(len(test_labels)):\n",
    "    test_tweet_preds.append(get_tweet_label_from_sents(idx, preds, test_tweet_sent_map))\n",
    "    \n",
    "predictions_.append(test_tweet_preds)\n",
    "truths_.append(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**\n",
    "\n",
    "From the above, we explored 4 exclusive options and generated evaluation results. We implement these 4 exclusive options following the order like:\n",
    "1. Preprocessing option: Remove stopwords in BOW from training set\n",
    "2. Tuning with best classifier param options: C=0.1, max_iter=100 in logistic regression classifier\n",
    "3. Lexicon option: Use all 4 lexicons \n",
    "4. Improvement option: Improment method 1 + improvement method 3\n",
    "\n",
    "Firstly, we draw the accuracy bar chart responding our 4 options plus the accuracy that is without any options and the accuracy that combined all options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmQAAAFCCAYAAABfDMEKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x384de110>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def accuracy_graph(preds, truths):\n",
    "    acc_list = []\n",
    "    for idx in xrange(len(preds)):\n",
    "        acc_list.append(accuracy_score(truths[idx], preds[idx]))\n",
    "        \n",
    "    plt.figure(figsize=(10,5))\n",
    "    p = plt.bar([num+1.0 for num in range(len(preds))], acc_list, 0.5, align='center')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy classifying test tweets with different options using logistic regression')\n",
    "    plt.ylim(0.48)\n",
    "    plt.xticks([num+1.0 for num in range(len(preds))], \n",
    "               ('no option','no stopwords', 'best-param', 'lexicon', 'improment'))\n",
    "    plt.show()\n",
    "\n",
    "accuracy_graph(predictions_, truths_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the chart above, if we take \"no-option\" bar as baseline, options that applies best classifier parameter and improvement methods improve the performance as expected. However, \"no stopwords\" bar and \"lexicon\" bar does not perform as expected like in development set where the \"no-option\" accuracy is 50.02% and \"stopwords + lexicon\" accuracy is 50.46%. \n",
    "\n",
    "From now on, we are confirmed that our improvement methods indeed improve the result. But we don't know if the \"no stopwords\" option and \"lexicon\" options really decrease the accuracy on test set, so for simplicity, we plot the accuracy with all options and options that only includes best classifier parameters and our improvement methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of two groups: 0.553403431101 (all options), 0.549529607084 (2+4 options)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmQAAAFCCAYAAABfDMEKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nnQiPGwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x38cb2fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "''' Final test of all major options:\n",
    "    1. Preprocessing option: Remove stopwords in BOW from training set\n",
    "    2. Tuning option: C=0.1, max_iter=100 in logistic regression classifier\n",
    "    3. Lexicon option: Use all 4 lexicons \n",
    "    4. Improvement option: Improment method 1 + improvement method 3\n",
    "    and control group that only have option 2 and 4.\n",
    "'''\n",
    "def accuracy_with_all_options(train_tweets, \n",
    "                              train_labels, \n",
    "                              test_tweets, \n",
    "                              test_labels, \n",
    "                              lexicon_tuples=[], \n",
    "                              stopwords=set()):\n",
    "    train_sent_dicts, train_sent_labels, _ = get_sent_dicts_label(train_tweets, \n",
    "                                                                  train_labels,  \n",
    "                                                                  lexicon_tuples,\n",
    "                                                                  stopwords=stopwords,\n",
    "                                                                 )\n",
    "    test_sent_dicts, _, test_tweet_sent_map = get_sent_dicts_label(test_tweets,\n",
    "                                                                  test_labels, \n",
    "                                                                  lexicon_tuples,\n",
    "                                                                  trainset=False,\n",
    "                                                                  )\n",
    "    vectorizor = DictVectorizer()\n",
    "    train_sent_x = vectorizer.fit_transform(train_sent_dicts)\n",
    "    test_sent_x = vectorizer.transform(test_sent_dicts)\n",
    "    # use best tuning options\n",
    "    final_clf = LogisticRegression(solver='lbfgs',\n",
    "                                      C=0.1,\n",
    "                                      max_iter=100,\n",
    "                                      multi_class='multinomial')\n",
    "\n",
    "    final_clf.fit(train_sent_x, train_sent_labels)\n",
    "    pred = final_clf.predict(test_sent_x)\n",
    "    test_tweet_preds = []\n",
    "    for idx in xrange(len(test_labels)):\n",
    "        test_tweet_preds.append(get_tweet_label_from_sents(idx, pred, test_tweet_sent_map))\n",
    "    \n",
    "    return accuracy_score(test_labels, test_tweet_preds), test_tweet_preds, test_labels\n",
    "\n",
    "acc_all_options, pred, labels = accuracy_with_all_options(train_tweets, train_labels, \n",
    "                                            test_tweets, test_labels,\n",
    "                                            lexicon_tuples[1:], \n",
    "                                            stopwords)\n",
    "predictions_.append(pred)\n",
    "truths_.append(labels)\n",
    "acc_control, pred, labels = accuracy_with_all_options(train_tweets, train_labels, \n",
    "                                                      test_tweets, test_labels)\n",
    "predictions_.append(pred)\n",
    "truths_.append(labels)\n",
    "\n",
    "acc_list = [acc_all_options, acc_control]\n",
    "print 'Accuracy of two groups: %s (all options), %s (2+4 options)' % (acc_list[0], acc_list[1])\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "p = plt.bar([num+1.0 for num in range(len(acc_list))], acc_list, 0.5, align='center')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy classifying test tweets with all options and control group having option 2 and 4')\n",
    "plt.ylim(0.5)\n",
    "plt.xticks([num+1.0 for num in range(len(acc_list))], \n",
    "            ('all option','2+4 option'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From now on, we still cannot tell which solution is better, because the results are so close and the risk is not only measured by accuracy, we want to view it on more generalized metrics, precision, recall, f1-score. \n",
    "\n",
    "Next, we produce the table (classification report) that contain precision, recall, f1-score with respect to each case and macro-averaged f1-score. Each result include a table and macro-averaged f1-score. Each result respond to a specific case. Cases are numbered as the following order and are described later with the same order. \n",
    "\n",
    "0. No options (baseline)\n",
    "1. Preprocessing option: Remove stopwords in BOW from training set\n",
    "2. Tuning with best classifier param options: C=0.1, max_iter=100 in logistic regression classifier\n",
    "3. Lexicon option: Use all 4 lexicons \n",
    "4. Improvement option: Improment method 1 + improvement method 3\n",
    "5. All obove (2+3+4+5) options \n",
    "6. Controlled options (3+5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "///////////////////////Case 1//////////////////////////\n",
      "Evaluation Table:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.37      0.43      0.40       290\n",
      "          0       0.43      0.63      0.51       625\n",
      "          1       0.75      0.46      0.57       892\n",
      "\n",
      "avg / total       0.58      0.51      0.52      1807\n",
      "\n",
      "Macro-averaged f1 score is: 0.491331916602\n",
      "///////////////////////Case 2//////////////////////////\n",
      "Evaluation Table:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.35      0.36      0.36       290\n",
      "          0       0.41      0.63      0.50       625\n",
      "          1       0.73      0.44      0.55       892\n",
      "\n",
      "avg / total       0.56      0.49      0.50      1807\n",
      "\n",
      "Macro-averaged f1 score is: 0.466989183405\n",
      "///////////////////////Case 3//////////////////////////\n",
      "Evaluation Table:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.38      0.33      0.35       290\n",
      "          0       0.43      0.70      0.53       625\n",
      "          1       0.77      0.45      0.57       892\n",
      "\n",
      "avg / total       0.59      0.52      0.52      1807\n",
      "\n",
      "Macro-averaged f1 score is: 0.485481006945\n",
      "///////////////////////Case 4//////////////////////////\n",
      "Evaluation Table:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.35      0.46      0.39       290\n",
      "          0       0.42      0.61      0.50       625\n",
      "          1       0.75      0.44      0.56       892\n",
      "\n",
      "avg / total       0.57      0.50      0.51      1807\n",
      "\n",
      "Macro-averaged f1 score is: 0.482804232804\n",
      "///////////////////////Case 5//////////////////////////\n",
      "Evaluation Table:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.37      0.41      0.39       290\n",
      "          0       0.44      0.49      0.47       625\n",
      "          1       0.70      0.61      0.65       892\n",
      "\n",
      "avg / total       0.56      0.54      0.55      1807\n",
      "\n",
      "Macro-averaged f1 score is: 0.502422286718\n",
      "///////////////////////Case 6//////////////////////////\n",
      "Evaluation Table:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.44      0.32      0.37       290\n",
      "          0       0.45      0.53      0.49       625\n",
      "          1       0.67      0.64      0.65       892\n",
      "\n",
      "avg / total       0.56      0.55      0.55      1807\n",
      "\n",
      "Macro-averaged f1 score is: 0.506233434992\n",
      "///////////////////////Case 7//////////////////////////\n",
      "Evaluation Table:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.40      0.33      0.36       290\n",
      "          0       0.45      0.53      0.49       625\n",
      "          1       0.68      0.64      0.66       892\n",
      "\n",
      "avg / total       0.55      0.55      0.55      1807\n",
      "\n",
      "Macro-averaged f1 score is: 0.501273414196\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "for idx in range(len(predictions_)):\n",
    "    print '///////////////////////Case %s//////////////////////////' % (idx+1)\n",
    "    print 'Evaluation Table:'\n",
    "    print classification_report(truths_[idx], predictions_[idx])\n",
    "    print 'Macro-averaged f1 score is: %s' % f1_score(truths_[idx], predictions_[idx], average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From now on, according to f1-score (micro & macro), \"case 5\" and \"case 6\" are the top 2 best. Since results of the best two cases are very close, it is not that convincing to select the best from the two. From my own opinion, if I am sure that the label represents polarity, I will use \"case 5\" as final model, because it adds polarity lexicon features.\n",
    "\n",
    "**Reflections**\n",
    "\n",
    "After the implementation of this assignment, I reviewed the preprocessing of NLP problem step by step:\n",
    "* Remove unwanted formatting\n",
    "* Segment sentences\n",
    "* Tokenize words\n",
    "* Normalize words (lemmatization)\n",
    "* Remove unwanted words (stemming)\n",
    "\n",
    "I also reinforce the understanding of text classification, such as using term frequency to represent documents, BOW feature transformation, building term-document matrix, etc. The process of comparison of different classifiers gives me direct experiences of how to better using sci-kit package. \n",
    "\n",
    "The exploration of using lexicon to analyse sentiment behind documents enhances my understanding of sentiment analtics capabilities, including how to generate lexicons with hands-on packages (like sentiwordnet), how to evaluate them. \n",
    "\n",
    "The final evaluation methods offer me potential angles to evaluate errors, especially multi-class classification problem and NLP related thinking method (like transforming document-based BOW features to sentence-based BOW features).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
